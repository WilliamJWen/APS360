{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bphECiUa9zw"
      },
      "source": [
        "# Lab 5: Spam Detection\n",
        "\n",
        "In this assignment, we will build a recurrent neural network to classify a SMS text message\n",
        "as \"spam\" or \"not spam\". In the process, you will\n",
        "    \n",
        "1. Clean and process text data for machine learning.\n",
        "2. Understand and implement a character-level recurrent neural network.\n",
        "3. Understand batching for a recurrent neural network, and develop custom Dataset and DataLoaders with collate_fn to implement RNN batching.\n",
        "\n",
        "### What to submit\n",
        "\n",
        "Submit a PDF file containing all your code, outputs, and write-up. You can produce a PDF of your Google Colab file by going to File > Print and then save as PDF. The Colab instructions have more information.\n",
        "\n",
        "Do not submit any other files produced by your code.\n",
        "\n",
        "Include a link to your colab file in your submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWiUqJJTa9z6"
      },
      "source": [
        "## Colab Link\n",
        "\n",
        "Include a link to your Colab file here. If you would like the TA to look at your\n",
        "Colab file in case your solutions are cut off, **please make sure that your Colab\n",
        "file is publicly accessible at the time of submission**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colab Link: https://colab.research.google.com/github/WilliamJWen/APS360/blob/main/lab/Lab5_Spam_Detection.ipynb"
      ],
      "metadata": {
        "id": "9FP125KItpXz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HgfNOUaPa9z8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0jLI9LBa90C"
      },
      "source": [
        "## Part 1. Data Cleaning [15 pt]\n",
        "\n",
        "We will be using the \"SMS Spam Collection Data Set\" available at http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
        "\n",
        "There is a link to download the \"Data Folder\" at the very top of the webpage. Download the zip file, unzip it, and upload the file `SMSSpamCollection` to Colab.    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SYNK-a1cuqbO",
        "outputId": "fc205722-07d7-487f-a704-e3ebba0605ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cp /content/drive/MyDrive/APS360/lab5/sms+spam+collection/SMSSpamCollection ."
      ],
      "metadata": {
        "id": "DGcoEQHPvFSk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSuF7C_Ga90E"
      },
      "source": [
        "### Part (a) [1 pt]\n",
        "\n",
        "Open up the file in Python, and print out one example of a spam SMS, and one example of a non-spam SMS.\n",
        "\n",
        "What is the label value for a spam message, and what is the label value for a non-spam message?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "I_IfXHeTa90F",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "e2ab150d-5d1e-4195-95f0-070988c99dcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
            "\n",
            "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
        "# ham\tU dun say so early hor... U c already then say...\n",
        "\n",
        "for line in open('SMSSpamCollection'):\n",
        "  first_word = line.split()[0]\n",
        "  if first_word == 'spam':\n",
        "    print(line)\n",
        "    break\n",
        "\n",
        "for line in open('SMSSpamCollection'):\n",
        "  first_word = line.split()[0]\n",
        "  if first_word == 'ham':\n",
        "    print(line)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "- The label value for a spam message is \"spam\".\n",
        "- The label value for a non-spam message is \"ham\"."
      ],
      "metadata": {
        "id": "bDuToN3XykNA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AukA6vMVa90d"
      },
      "source": [
        "### Part (b) [1 pt]\n",
        "\n",
        "How many spam messages are there in the data set?\n",
        "How many non-spam messages are there in the data set?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LgsqyemVa90e",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "bb1606e3-126b-435e-d07c-205a692a2f4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 747 spam messages in the data set.\n",
            "There are 4827 non-spam messages in the data set.\n"
          ]
        }
      ],
      "source": [
        "spam_count = 0\n",
        "ham_count = 0\n",
        "for line in open('SMSSpamCollection'):\n",
        "  first_word = line.split()[0]\n",
        "  if first_word == 'spam':\n",
        "    spam_count += 1\n",
        "  elif first_word == 'ham':\n",
        "    ham_count += 1\n",
        "print(f'There are {spam_count} spam messages in the data set.')\n",
        "print(f'There are {ham_count} non-spam messages in the data set.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1WXxVt6a90h"
      },
      "source": [
        "### Part (c) [4 pt]\n",
        "\n",
        "load and parse the data into two lists: sequences and labels. Create character-level stoi and itos dictionaries. Reserve the index 0 for padding. Convert the sequences to list of character ids using stoi dictionary and convert the labels to a list of 0s and 1s by assinging class \"ham\" to 0 and class \"spam\" to 1."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "characters = list(string.ascii_letters + string.digits + string.punctuation + ' ')\n",
        "print(characters)\n",
        "stoi = {'padding': 0}\n",
        "itos = {0: 'padding'}\n",
        "for idx, char in enumerate(characters):\n",
        "  stoi[char] = idx + 1\n",
        "  itos[idx + 1] = char\n",
        "print(stoi)\n",
        "print(itos)\n"
      ],
      "metadata": {
        "id": "R7c1BwlH211B",
        "outputId": "7f3032d3-4203-4d0f-c0ba-fdd6a6f938e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', ' ']\n",
            "{'padding': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, '0': 53, '1': 54, '2': 55, '3': 56, '4': 57, '5': 58, '6': 59, '7': 60, '8': 61, '9': 62, '!': 63, '\"': 64, '#': 65, '$': 66, '%': 67, '&': 68, \"'\": 69, '(': 70, ')': 71, '*': 72, '+': 73, ',': 74, '-': 75, '.': 76, '/': 77, ':': 78, ';': 79, '<': 80, '=': 81, '>': 82, '?': 83, '@': 84, '[': 85, '\\\\': 86, ']': 87, '^': 88, '_': 89, '`': 90, '{': 91, '|': 92, '}': 93, '~': 94, ' ': 95}\n",
            "{0: 'padding', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 27: 'A', 28: 'B', 29: 'C', 30: 'D', 31: 'E', 32: 'F', 33: 'G', 34: 'H', 35: 'I', 36: 'J', 37: 'K', 38: 'L', 39: 'M', 40: 'N', 41: 'O', 42: 'P', 43: 'Q', 44: 'R', 45: 'S', 46: 'T', 47: 'U', 48: 'V', 49: 'W', 50: 'X', 51: 'Y', 52: 'Z', 53: '0', 54: '1', 55: '2', 56: '3', 57: '4', 58: '5', 59: '6', 60: '7', 61: '8', 62: '9', 63: '!', 64: '\"', 65: '#', 66: '$', 67: '%', 68: '&', 69: \"'\", 70: '(', 71: ')', 72: '*', 73: '+', 74: ',', 75: '-', 76: '.', 77: '/', 78: ':', 79: ';', 80: '<', 81: '=', 82: '>', 83: '?', 84: '@', 85: '[', 86: '\\\\', 87: ']', 88: '^', 89: '_', 90: '`', 91: '{', 92: '|', 93: '}', 94: '~', 95: ' '}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mhnz8Nk-a90i",
        "tags": [],
        "outputId": "c101af14-8244-4ebf-87a2-8351779c76db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 1, 0, 0]\n",
            "[[33, 15, 95, 21, 14, 20, 9, 12, 95, 10, 21, 18, 15, 14, 7, 95, 16, 15, 9, 14, 20, 74, 95, 3, 18, 1, 26, 25, 76, 76, 95, 27, 22, 1, 9, 12, 1, 2, 12, 5, 95, 15, 14, 12, 25, 95, 9, 14, 95, 2, 21, 7, 9, 19, 95, 14, 95, 7, 18, 5, 1, 20, 95, 23, 15, 18, 12, 4, 95, 12, 1, 95, 5, 95, 2, 21, 6, 6, 5, 20, 76, 76, 76, 95, 29, 9, 14, 5, 95, 20, 8, 5, 18, 5, 95, 7, 15, 20, 95, 1, 13, 15, 18, 5, 95, 23, 1, 20, 76, 76, 76], [41, 11, 95, 12, 1, 18, 76, 76, 76, 95, 36, 15, 11, 9, 14, 7, 95, 23, 9, 6, 95, 21, 95, 15, 14, 9, 76, 76, 76], [32, 18, 5, 5, 95, 5, 14, 20, 18, 25, 95, 9, 14, 95, 55, 95, 1, 95, 23, 11, 12, 25, 95, 3, 15, 13, 16, 95, 20, 15, 95, 23, 9, 14, 95, 32, 27, 95, 29, 21, 16, 95, 6, 9, 14, 1, 12, 95, 20, 11, 20, 19, 95, 55, 54, 19, 20, 95, 39, 1, 25, 95, 55, 53, 53, 58, 76, 95, 46, 5, 24, 20, 95, 32, 27, 95, 20, 15, 95, 61, 60, 54, 55, 54, 95, 20, 15, 95, 18, 5, 3, 5, 9, 22, 5, 95, 5, 14, 20, 18, 25, 95, 17, 21, 5, 19, 20, 9, 15, 14, 70, 19, 20, 4, 95, 20, 24, 20, 95, 18, 1, 20, 5, 71, 46, 68, 29, 69, 19, 95, 1, 16, 16, 12, 25, 95, 53, 61, 57, 58, 55, 61, 54, 53, 53, 60, 58, 15, 22, 5, 18, 54, 61, 69, 19], [47, 95, 4, 21, 14, 95, 19, 1, 25, 95, 19, 15, 95, 5, 1, 18, 12, 25, 95, 8, 15, 18, 76, 76, 76, 95, 47, 95, 3, 95, 1, 12, 18, 5, 1, 4, 25, 95, 20, 8, 5, 14, 95, 19, 1, 25, 76, 76, 76], [40, 1, 8, 95, 35, 95, 4, 15, 14, 69, 20, 95, 20, 8, 9, 14, 11, 95, 8, 5, 95, 7, 15, 5, 19, 95, 20, 15, 95, 21, 19, 6, 74, 95, 8, 5, 95, 12, 9, 22, 5, 19, 95, 1, 18, 15, 21, 14, 4, 95, 8, 5, 18, 5, 95, 20, 8, 15, 21, 7, 8]]\n"
          ]
        }
      ],
      "source": [
        "sequences = []\n",
        "labels = []\n",
        "\n",
        "for line in open('SMSSpamCollection'):\n",
        "  line = line.strip() # remove '\\n' and trailing white spaces\n",
        "  label, sentence = line.split('\\t')\n",
        "\n",
        "  if label == 'ham':\n",
        "    labels.append(0)\n",
        "  elif label == 'spam':\n",
        "    labels.append(1)\n",
        "  else:\n",
        "    raise ValueError(f'Unknown label: {label}')\n",
        "  sequence = []\n",
        "  for char in sentence:\n",
        "    if char not in stoi:\n",
        "      stoi[char] = len(stoi)\n",
        "      itos[len(itos)] = char\n",
        "    sequence.append(stoi[char])\n",
        "  sequences.append(sequence)\n",
        "print(labels[:5])\n",
        "print(sequences[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie_D0bv9a90k"
      },
      "source": [
        "### Part (d) [4 pt]\n",
        "\n",
        "Use train_test_split function from sklearn (https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.train_test_split.html) to split the data indices into `train`, `valid`, and `test`. Use a 60-20-20 split.\n",
        "\n",
        "You saw in part (b) that there are many more non-spam messages than spam messages. This **imbalance** in our training data will be problematic for training. We can fix this disparity by duplicating spam messages in the training set, so that the training set is roughly balanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6mktDVJisi63"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "indices = list(range(len(sequences)))\n",
        "train_index, temp_index = train_test_split(\n",
        "    indices, test_size=0.4, random_state=42, stratify=labels)\n",
        "val_index, test_index = train_test_split(\n",
        "    temp_index, test_size=0.5, random_state=42, stratify=[labels[i] for i in temp_index] )\n",
        "\n",
        "# x: sequences, y: labels\n",
        "train_x = [sequences[idx] for idx in train_index]\n",
        "train_y = [labels[idx] for idx in train_index]\n",
        "val_x = [sequences[idx] for idx in val_index]\n",
        "val_y = [labels[idx] for idx in val_index]\n",
        "test_x = [sequences[idx] for idx in test_index]\n",
        "test_y = [labels[idx] for idx in test_index]\n",
        "\n",
        "#Balance the train classes\n",
        "train_spam = []\n",
        "for idx, item in enumerate(train_x):\n",
        "    if train_y[idx] == 1:\n",
        "        train_spam.append(item)\n",
        "# duplicate each spam message 6 more times\n",
        "train_x = train_x + train_spam * 6\n",
        "train_y = train_y + [1] * (len(train_spam) * 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6nP0Ks_a90o"
      },
      "source": [
        "### Part (e) [4 pt]\n",
        "\n",
        "Since each sequence has a different length, we cannot use the default DataLoader. We need to change the DataLoader such that it can pad differnt sequence sizes within the batch. To do this, we need to introduce a **collate_fn** to the DataLoader such that it uses **pad_sequence** function (https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html) to pad the sequences within the batch to the same size.\n",
        "\n",
        "We also need a custom Dataset class to return a pair of sequence and label for each example. Complete the code below to address these.\n",
        "\n",
        "Hint:\n",
        "- https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
        "- https://plainenglish.io/blog/understanding-collate-fn-in-pytorch-f9d1742647d3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "FWvx9_rka90p",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]\n",
        "\n",
        "def collate_sequences(batch):\n",
        "  sequences, labels = zip(*batch)\n",
        "  sequences = torch.tensor(sequences)\n",
        "  padded_sequences = pad_sequence(sequences, batch_first=True)\n",
        "  labels = torch.tensor(labels)\n",
        "\n",
        "  return padded_sequences, labels\n",
        "\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset=MyDataset(train_x, train_y), batch_size=32, shuffle=True, collate_fn=collate_sequences)\n",
        "val_loader = DataLoader(dataset=MyDataset(val_x, val_y), batch_size=32, shuffle=False, collate_fn=collate_sequences)\n",
        "test_loader = DataLoader(dataset=MyDataset(test_x, test_y), batch_size=32, shuffle=False, collate_fn=collate_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx, bach in enumerate(train_loader):\n",
        "  print(bach)\n",
        "  break"
      ],
      "metadata": {
        "id": "_IjpZyoO14cB",
        "outputId": "ee6bffe9-cc4f-4368-be55-0b60b8bdb861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "expected sequence of length 70 at dim 1 (got 76)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-108eb01b4d37>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-d6f290a5b053>\u001b[0m in \u001b[0;36mcollate_sequences\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollate_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mpadded_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 70 at dim 1 (got 76)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "metadata": {
        "id": "vqa1mHZM1WFW",
        "outputId": "495af990-41e0-494f-d615-910f9d2419eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "189"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_loader)"
      ],
      "metadata": {
        "id": "GDmNgluP1cEy",
        "outputId": "e7134b58-3c1b-445a-e141-85ad72a884ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_loader)"
      ],
      "metadata": {
        "id": "sgNO4ysE1gMD",
        "outputId": "5bf4cfa0-4a67-450f-9acd-200aa06633fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x796530d9fd10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff5CNk7Qa90y"
      },
      "source": [
        "### Part (f) [1 pt]\n",
        "\n",
        "Take a look at 10 batches in `train_loader`. What is the maximum length of the\n",
        "input sequence in each batch? How many `<pad>` tokens are used in each of the 10\n",
        "batches?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwz-rOaha902",
        "tags": []
      },
      "outputs": [],
      "source": [
        "for batch in train_loader:\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7HnqP6_a904"
      },
      "source": [
        "## Part 2. Model Building [8 pt]\n",
        "\n",
        "Build a recurrent neural network model, using an architecture of your choosing.\n",
        "Use the one-hot embedding of each character as input to your recurrent network.\n",
        "Use one or more fully-connected layers to make the prediction based on your\n",
        "recurrent network output.\n",
        "\n",
        "Instead of using the RNN output value for the final token, another often used\n",
        "strategy is to max-pool over the entire output array. That is, instead of calling\n",
        "something like:\n",
        "\n",
        "```\n",
        "out, _ = self.rnn(x)\n",
        "self.fc(out[:, -1, :])\n",
        "```\n",
        "\n",
        "where `self.rnn` is an `nn.RNN`, `nn.GRU`, or `nn.LSTM` module, and `self.fc` is a\n",
        "fully-connected\n",
        "layer, we use:\n",
        "\n",
        "```\n",
        "out, _ = self.rnn(x)\n",
        "self.fc(torch.max(out, dim=1)[0])\n",
        "```\n",
        "\n",
        "This works reasonably in practice. An even better alternative is to concatenate the\n",
        "max-pooling and average-pooling of the RNN outputs:\n",
        "\n",
        "```\n",
        "out, _ = self.rnn(x)\n",
        "out = torch.cat([torch.max(out, dim=1)[0],\n",
        "                 torch.mean(out, dim=1)], dim=1)\n",
        "self.fc(out)\n",
        "```\n",
        "\n",
        "We encourage you to try out all these options. The way you pool the RNN outputs\n",
        "is one of the \"hyperparameters\" that you can choose to tune later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jHl1p_Wwa905",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# You might find this code helpful for obtaining\n",
        "# PyTorch one-hot vectors.\n",
        "\n",
        "ident = torch.eye(10)\n",
        "print(ident[0]) # one-hot vector\n",
        "print(ident[1]) # one-hot vector\n",
        "x = torch.tensor([[1, 2], [3, 4]])\n",
        "print(ident[x]) # one-hot vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4LTQ7zFka909",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKIYPl_Ba90_"
      },
      "source": [
        "## Part 3. Training [16 pt]\n",
        "\n",
        "### Part (a) [4 pt]\n",
        "\n",
        "Complete the `get_accuracy` function, which will compute the\n",
        "accuracy (rate) of your model across a dataset (e.g. validation set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pvNfhGD6a91A",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, data):\n",
        "    \"\"\" Compute the accuracy of the `model` across a dataset `data`\n",
        "\n",
        "    Example usage:\n",
        "\n",
        "    >>> model = MyRNN() # to be defined\n",
        "    >>> get_accuracy(model, valid) # the variable `valid` is from above\n",
        "    \"\"\"\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlxlcAC1a91C"
      },
      "source": [
        "### Part (b) [4 pt]\n",
        "\n",
        "Train your model. Plot the training curve of your final model.\n",
        "Your training curve should have the training/validation loss and\n",
        "accuracy plotted periodically.\n",
        "\n",
        "Note: Not all of your batches will have the same batch size.\n",
        "In particular, if your training set does not divide evenly by\n",
        "your batch size, there will be a batch that is smaller than\n",
        "the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CVtf7CJCa91D",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE3eRkDAa91F"
      },
      "source": [
        "### Part (c) [4 pt]\n",
        "\n",
        "Choose at least 4 hyperparameters to tune. Explain how you tuned the hyperparameters.\n",
        "You don't need to include your training curve for every model you trained.\n",
        "Instead, explain what hyperparemters you tuned, what the best validation accuracy was,\n",
        "and the reasoning behind the hyperparameter decisions you made.\n",
        "\n",
        "For this assignment, you should tune more than just your learning rate and epoch.\n",
        "Choose at least 2 hyperparameters that are unrelated to the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "A2GEWfDca91G",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7DY56rKa91I"
      },
      "source": [
        "### Part (d) [2 pt]\n",
        "\n",
        "Before we deploy a machine learning model, we usually want to have a better understanding\n",
        "of how our model performs beyond its validation accuracy. An important metric to track is\n",
        "*how well our model performs in certain subsets of the data*.\n",
        "\n",
        "In particular, what is the model's error rate amongst data with negative labels?\n",
        "This is called the **false positive rate**.\n",
        "\n",
        "What about the model's error rate amongst data with positive labels?\n",
        "This is called the **false negative rate**.\n",
        "\n",
        "Report your final model's false positive and false negative rate across the\n",
        "validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7ggbQSdba91J",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1iRteb3a91O"
      },
      "source": [
        "### Part (e) [2 pt]\n",
        "\n",
        "The impact of a false positive vs a false negative can be drastically different.\n",
        "If our spam detection algorithm was deployed on your phone, what is the impact\n",
        "of a false positive on the phone's user? What is the impact of a false negative?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hFLUOJTGa91Q",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gznefulsa91V"
      },
      "source": [
        "## Part 4. Evaluation [11 pt]\n",
        "\n",
        "### Part (a) [1 pt]\n",
        "\n",
        "Report the final test accuracy of your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "D5L5D-A1a91W",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hjmd8rca91Y"
      },
      "source": [
        "### Part (b) [3 pt]\n",
        "\n",
        "Report the false positive rate and false negative rate of your model across the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GFiAKztJa91Z",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jGHtQFpa91b"
      },
      "source": [
        "### Part (c) [3 pt]\n",
        "\n",
        "What is your model's prediction of the **probability** that\n",
        "the SMS message \"machine learning is sooo cool!\" is spam?\n",
        "\n",
        "Hint: To begin, use `stoi` to look up the index\n",
        "of each character in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "h_2nSJq8a91b",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "msg = \"machine learning is sooo cool!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD1zgYJpa91f"
      },
      "source": [
        "### Part (d) [4 pt]\n",
        "\n",
        "Do you think detecting spam is an easy or difficult task?\n",
        "\n",
        "Since machine learning models are expensive to train and deploy, it is very\n",
        "important to compare our models against baseline models: a simple\n",
        "model that is easy to build and inexpensive to run that we can compare our\n",
        "recurrent neural network model against.\n",
        "\n",
        "Explain how you might build a simple baseline model. This baseline model\n",
        "can be a simple neural network (with very few weights), a hand-written algorithm,\n",
        "or any other strategy that is easy to build and test.\n",
        "\n",
        "**Do not actually build a baseline model. Instead, provide instructions on\n",
        "how to build it.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LTndp-IOa91g",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Lab 5 - Spam Detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}