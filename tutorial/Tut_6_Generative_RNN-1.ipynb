{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO4OksS4R1U9"
      },
      "source": [
        "# Tutorial - Generative Recurrent Neural Networks\n",
        "\n",
        "Last time we discussed using recurrent neural networks to make predictions about sequences. In particular, we treated tweets as a **sequence** of words. Since tweets can have a variable number of words, we needed an architecture that can take variable-sized sequences as input.\n",
        "\n",
        "This time, we will use recurrent neural networks to **generate** sequences.\n",
        "Generating sequences is more involved compared to making predictions about\n",
        "sequences. However, it is a very interesting task, and many students chose\n",
        "sequence-generation tasks for their projects.\n",
        "\n",
        "Much of today's content is an adaptation of the \"Practical PyTorch\" GitHub\n",
        "repository [1].\n",
        "\n",
        "[1] https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiwPm7atR1VC"
      },
      "source": [
        "## Review\n",
        "\n",
        "In recurrent neural networks, the input sequence is broken down into tokens. We could choose whether to tokenize based on words, or based on characters. The representation of each token (GloVe or one-hot) is processed by the RNN one step at a time to update the hidden (or context) state.\n",
        "\n",
        "In a predictive RNN, the value of the hidden states  is a representation of **all the text that was processed thus far**. Similarly, in a generative RNN, The value of the hidden state will be a representation of **all the text that still needs to be generated**. We will use this hidden state to produce the sequence, one token at a time.\n",
        "\n",
        "Similar to the last tutorial we will break up the problem of generating text\n",
        "to generating one token at a time.\n",
        "\n",
        "We will do so with the help of two functions:\n",
        "\n",
        "1. We need to be able to generate the *next* token, given the current\n",
        "   hidden state. In practice, we get a probability distribution over\n",
        "   the next token, and sample from that probability distribution.\n",
        "2. We need to be able to update the hidden state somehow. To do so,\n",
        "   we need two pieces of information: the old hidden state, and the actual\n",
        "   token that was generated in the previous step. The actual token generated\n",
        "   will inform the subsequent tokens.\n",
        "\n",
        "We will repeat both functions until a special \"END OF SEQUENCE\" token is\n",
        "generated.\n",
        "\n",
        "Note that there are several tricky things that we will have to figure out.\n",
        "For example, how do we actually sample the actual token from the probability\n",
        "distribution over tokens? What would we do during training, and how might\n",
        "that be different from during testing/evaluation? We will answer those\n",
        "questions as we implement the RNN.\n",
        "\n",
        "For now, let's start with our training data.\n",
        "\n",
        "## Data: Donald Trump's Tweets from 2018\n",
        "\n",
        "The training set we use is a collection of Donald Trump's tweets from 2018.\n",
        "We will only use tweets that are 140 characters or shorter, and tweets\n",
        "that contains more than just a URL.\n",
        "Since tweets often contain creative spelling and numbers, and upper vs. lower\n",
        "case characters are read very differently, we will use a character-level RNN.\n",
        "\n",
        "To start, let us load the trump.csv file to Google Colab and provide access to the drive. The file can be obtained from Quercus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UUKaz67w3hCN",
        "outputId": "9ca2155e-ecb7-4c2f-fa90-dcd811a6eb74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torchtext==0.6"
      ],
      "metadata": {
        "id": "-nODV94R2-ZY",
        "outputId": "ce4c1d63-0061-4f96-cbea-4b705d3ef2c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.6\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6) (2.32.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6) (2.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6) (1.17.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6) (0.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.6) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.6) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.6) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.6) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->torchtext==0.6)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->torchtext==0.6)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->torchtext==0.6)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->torchtext==0.6)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->torchtext==0.6)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->torchtext==0.6)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->torchtext==0.6)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->torchtext==0.6)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->torchtext==0.6)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->torchtext==0.6)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchtext==0.6) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchtext==0.6) (3.0.2)\n",
            "Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchtext\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchtext-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "woTmhzIzR1VD",
        "outputId": "7eb60c8f-a484-4db3-cc7e-b33b5305ec72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22402"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "# file location (make sure to use your file location)\n",
        "file_dir = '/content/drive/MyDrive/APS360/tut6/'\n",
        "\n",
        "tweets = list(line[0] for line in csv.reader(open(file_dir + 'trump.csv')))\n",
        "len(tweets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGEHJRNcR1VG"
      },
      "source": [
        "There are over 20000 tweets in this collection.\n",
        "Let's look at a few of them, just to get a sense of the kind of text\n",
        "we're dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HhauODvnR1VH",
        "outputId": "7726bcae-c376-42c0-833b-fb68736d5696",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "God Bless the people of Venezuela!\n",
            "It was my honor. THANK YOU! https://t.co/1LvqbRQ1bi\n",
            "Nobody but Donald Trump will save Israel. You are wasting your time with these politicians and political clowns. Best! #SheldonAdelson\n"
          ]
        }
      ],
      "source": [
        "print(tweets[100])\n",
        "print(tweets[1000])\n",
        "print(tweets[10000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qksavJr_R1VK"
      },
      "source": [
        "## Generating One Tweet\n",
        "\n",
        "Normally, when we build a new machine learning model, we want to make sure\n",
        "that our model can overfit. To that end, we will first build a neural network\n",
        "that can generate _one_ tweet really well. We can choose any tweet (or any other text) we want. Let's choose to build an RNN that generates `tweet[100]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u-RT-m4-R1VL",
        "outputId": "979dde34-81ad-4d1b-e71c-16e8049fe5bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "God Bless the people of Venezuela!\n",
            "34\n"
          ]
        }
      ],
      "source": [
        "tweet = tweets[100]\n",
        "print(tweet)\n",
        "print(len(tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsdHRV1uR1VP"
      },
      "source": [
        "First, we will need to encode this tweet using a one-hot encoding.\n",
        "We'll build dictionary mappings\n",
        "from the character to the index of that character (a unique integer identifier),\n",
        "and from the index to the character. We'll use the same naming scheme that `torchtext`\n",
        "uses (`stoi` and `itos`).\n",
        "\n",
        "For simplicity, we'll work with a limited vocabulary containing\n",
        "just the characters in `tweet[100]`, plus two special tokens:\n",
        "\n",
        "- `<EOS>` represents \"End of String\", which we'll append to the end of our tweet.\n",
        "  Since tweets are variable-length, this is a way for the RNN to signal\n",
        "  that the entire sequence has been generated.\n",
        "- `<BOS>` represents \"Beginning of String\", which we'll prepend to the beginning of\n",
        "  our tweet. This is the first token that we will feed into the RNN.\n",
        "\n",
        "The way we use these special tokens will become more clear as we build the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "InvJaRXsR1VP"
      },
      "outputs": [],
      "source": [
        "vocab = list(set(tweet)) + [\"<BOS>\", \"<EOS>\"]\n",
        "vocab_stoi = {s: i for i, s in enumerate(vocab)} # String to index\n",
        "vocab_itos = {i: s for i, s in enumerate(vocab)} # Index to string\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r4i3C1qs5Rrt",
        "outputId": "8f300e3d-a5b6-4dc6-af44-509675b04a4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab\n",
            "['d', 'V', 'l', '!', 's', 'a', 'o', 'e', 'z', 'h', 'f', 'G', 'u', 't', 'n', ' ', 'p', 'B', '<BOS>', '<EOS>']\n",
            "STOI\n",
            "{'d': 0, 'V': 1, 'l': 2, '!': 3, 's': 4, 'a': 5, 'o': 6, 'e': 7, 'z': 8, 'h': 9, 'f': 10, 'G': 11, 'u': 12, 't': 13, 'n': 14, ' ': 15, 'p': 16, 'B': 17, '<BOS>': 18, '<EOS>': 19}\n",
            "ITOS\n",
            "{0: 'd', 1: 'V', 2: 'l', 3: '!', 4: 's', 5: 'a', 6: 'o', 7: 'e', 8: 'z', 9: 'h', 10: 'f', 11: 'G', 12: 'u', 13: 't', 14: 'n', 15: ' ', 16: 'p', 17: 'B', 18: '<BOS>', 19: '<EOS>'}\n",
            "Size\n",
            "20\n"
          ]
        }
      ],
      "source": [
        "print(\"Vocab\")\n",
        "print(vocab)\n",
        "print(\"STOI\")\n",
        "print(vocab_stoi)\n",
        "print(\"ITOS\")\n",
        "print(vocab_itos)\n",
        "print(\"Size\")\n",
        "print(vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of string -> index\n",
        "print(vocab_stoi[\"s\"])\n",
        "# Example of index -> string\n",
        "print(vocab_itos[17])"
      ],
      "metadata": {
        "id": "kOCq-KHZgYDM",
        "outputId": "0b6e2bf6-d82a-44b0-dda8-e8d79654f4a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXknqF8gR1VS"
      },
      "source": [
        "Now that we have our vocabulary, we can build the PyTorch model\n",
        "for this problem.\n",
        "The actual model is not as complex as you might think. We actually\n",
        "already learned about all the components that we need. (Using and training\n",
        "the model is the hard part)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rZiQsDFjR1U_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XCl7ORcrR1VT"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
        "        super(TextGenerator, self).__init__()\n",
        "\n",
        "        # identiy matrix for generating one-hot vectors\n",
        "        self.ident = torch.eye(vocab_size)\n",
        "\n",
        "        # recurrent neural network\n",
        "        self.rnn = nn.GRU(vocab_size, hidden_size, n_layers, batch_first=True) ## Gated Recurrent unit\n",
        "\n",
        "        # a fully-connect layer that outputs a distribution over\n",
        "        # the next token, given the RNN output\n",
        "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inp, hidden=None):\n",
        "        inp = self.ident[inp]                  # generate one-hot vectors of input\n",
        "        output, hidden = self.rnn(inp, hidden) # get the next output and hidden state\n",
        "        output = self.decoder(output)          # predict distribution over next tokens\n",
        "        return output, hidden\n",
        "\n",
        "model = TextGenerator(vocab_size, hidden_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87pVghfCR1VV"
      },
      "source": [
        "## Training with Teacher Forcing\n",
        "\n",
        "At a very high level, we want our RNN model to have a high probability\n",
        "of generating the tweet. An RNN model generates text\n",
        "one character at a time based on the hidden state value.\n",
        "At each time step, we will check whether the model generated the\n",
        "correct character. That is, at each time step,\n",
        "we are trying to select the correct next character out of all the\n",
        "characters in our vocabulary. Recall that this problem is a multi-class\n",
        "classification problem, and we can use Cross-Entropy loss to train our\n",
        "network to become better at this type of problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OI8aoJRpR1VX"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvgI1WQ4R1VZ"
      },
      "source": [
        "However, we don't just have a single multi-class classification problem.\n",
        "Instead, we have **one classification problem per time-step** (per token)!\n",
        "So, how do we predict the first token in the sequence?\n",
        "How do we predict the second token in the sequence?\n",
        "\n",
        "To help you understand what happens durign RNN training, we'll start with\n",
        "inefficient training code that shows you what happens step-by-step. We'll\n",
        "start with computing the loss for the first token generated, then the second token,\n",
        "and so on.\n",
        "Later on, we'll switch to a simpler and more performant version of the code.\n",
        "\n",
        "So, let's start with the first classification problem: the problem of generating\n",
        "the **first** token (`tweet[0]`).\n",
        "\n",
        "To generate the first token, we'll feed the RNN network (with an initial, empty\n",
        "hidden state) the \"<BOS>\" token. Then, the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "p6dTQqfKR1Va",
        "outputId": "6de4f286-a194-4d3e-a636-842c9faedaea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18.])\n"
          ]
        }
      ],
      "source": [
        "# First state is the \"\"\n",
        "bos_input = torch.Tensor([vocab_stoi[\"<BOS>\"]])\n",
        "print(bos_input)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bos_input.shape, type(bos_input))\n",
        "bos_input = bos_input.long()\n",
        "print(bos_input.shape, type(bos_input))\n",
        "bos_input = bos_input.unsqueeze(0)\n",
        "print(bos_input.shape, type(bos_input))\n"
      ],
      "metadata": {
        "id": "ePJ5a3L3re6I",
        "outputId": "af935ee4-a69b-43a2-c730-2f225973263a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1]) <class 'torch.Tensor'>\n",
            "torch.Size([1]) <class 'torch.Tensor'>\n",
            "torch.Size([1, 1]) <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output, hidden = model(bos_input, hidden=None)\n",
        "print(\"Output for first token - Hidden state 0\")\n",
        "print(output) # distribution over the first token\n",
        "print()\n",
        "print(\"Hidden state:\")\n",
        "print(hidden)\n",
        "print(hidden.shape)\n",
        "# It is not by chance that the output is 20 dimensional - same length as the vocabulary"
      ],
      "metadata": {
        "id": "tobh7fglrt2o",
        "outputId": "1b5330cb-a357-4774-f5e0-0e0928b289e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output for first token - Hidden state 0\n",
            "tensor([[[ 0.0642, -0.1402, -0.0681,  0.0219, -0.1105,  0.0041,  0.0655,\n",
            "          -0.0715, -0.0222, -0.1096,  0.0453,  0.1179,  0.0894, -0.0363,\n",
            "          -0.0339, -0.0599,  0.0808, -0.0526, -0.1597,  0.0347]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "\n",
            "Hidden state:\n",
            "tensor([[[ 3.1157e-03, -5.1094e-02, -2.1531e-02,  1.7843e-02, -5.8037e-02,\n",
            "          -2.0454e-03,  2.2216e-02, -5.6880e-02, -3.5258e-04, -9.3137e-04,\n",
            "           3.8888e-03,  8.5791e-02, -3.2170e-02, -7.7363e-02, -3.0693e-02,\n",
            "           2.4126e-02,  6.0290e-02, -4.9565e-02,  1.0500e-03,  1.7272e-02,\n",
            "          -2.2079e-02,  3.4342e-02, -2.4378e-02, -5.2401e-02, -2.6204e-02,\n",
            "           4.2237e-02,  1.1823e-02,  3.9661e-02, -1.4478e-02,  3.0761e-02,\n",
            "           2.5878e-02, -4.9325e-02, -4.1011e-02,  4.3148e-02, -7.0195e-02,\n",
            "          -8.9750e-02,  1.1353e-01,  6.6081e-02, -1.0613e-01, -4.0260e-02,\n",
            "           7.0838e-02,  2.3377e-02, -1.1878e-03, -1.7344e-02, -4.3672e-02,\n",
            "          -9.5209e-02, -1.0012e-01, -8.2242e-03, -2.3332e-02,  4.5906e-02,\n",
            "          -1.0863e-01,  7.1618e-02, -5.5823e-03, -1.3255e-02,  4.6469e-02,\n",
            "           7.7334e-02,  2.6451e-05, -1.0295e-01, -3.4905e-02, -2.0885e-02,\n",
            "          -2.1031e-02, -4.6606e-02, -9.3898e-02, -4.8096e-02]]],\n",
            "       grad_fn=<StackBackward0>)\n",
            "torch.Size([1, 1, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9pLljlLGiw52",
        "outputId": "29ed8527-cb08-4c19-ff72-683f0f82bf87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[18]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "bos_input"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet"
      ],
      "metadata": {
        "id": "OyK48qidrGax",
        "outputId": "60a90385-972e-4192-ade9-de30dae8e76e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'God Bless the people of Venezuela!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet[0]"
      ],
      "metadata": {
        "id": "2NrhMjpwrVqL",
        "outputId": "a0dfb435-a435-45ee-8dd7-2bccd04307be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'G'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVmBrFLVR1Vd"
      },
      "source": [
        "We can compute the loss using `criterion`. Since the model is untrained,\n",
        "the loss is expected to be high. (For now, we won't do anything\n",
        "with this loss, and omit the backward pass.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BN_FopQYR1Ve",
        "outputId": "259632ca-a187-4a60-dbd1-5e024d957a67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.8639, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "target = torch.Tensor([vocab_stoi[tweet[0]]]).long().unsqueeze(0)\n",
        "criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
        "          target.reshape(-1))             # reshape to 1D tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "L8M72rsz66Ar",
        "outputId": "f4144905-b6e3-4d40-b743-b2b068fbdabe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[11]])\n",
            "G\n",
            "tensor([[[ 0.0642, -0.1402, -0.0681,  0.0219, -0.1105,  0.0041,  0.0655,\n",
            "          -0.0715, -0.0222, -0.1096,  0.0453,  0.1179,  0.0894, -0.0363,\n",
            "          -0.0339, -0.0599,  0.0808, -0.0526, -0.1597,  0.0347]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "tensor([[ 0.0642, -0.1402, -0.0681,  0.0219, -0.1105,  0.0041,  0.0655, -0.0715,\n",
            "         -0.0222, -0.1096,  0.0453,  0.1179,  0.0894, -0.0363, -0.0339, -0.0599,\n",
            "          0.0808, -0.0526, -0.1597,  0.0347]], grad_fn=<ViewBackward0>)\n",
            "tensor([11])\n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "  print(target)\n",
        "  print(vocab_itos[int(target[0][0])])\n",
        "  print(output)\n",
        "  print(output.reshape(-1, vocab_size))\n",
        "  print(target.reshape(-1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int(target[0][0])"
      ],
      "metadata": {
        "id": "wdSVzQ7bsVbs",
        "outputId": "20d4c3b9-fa52-4b6a-98f2-60c15b7babd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr65KC4VR1Vg"
      },
      "source": [
        "Now, we need to update the hidden state and generate a prediction\n",
        "for the next token. To do so, **we need to provide the current token to\n",
        "the RNN**. We already said that during test time, we'll need to sample\n",
        "from the predicted probabilty over tokens that the neural network\n",
        "just generated.\n",
        "\n",
        "Right now, we can do something better: we can **use the ground-truth,\n",
        "actual target token**. This technique is called **teacher-forcing**,\n",
        "and generally speeds up training. The reason is that right now,\n",
        "since our model does not perform well, the predicted probability\n",
        "distribution is pretty far from the ground truth. So, it is very,\n",
        "very difficult for the neural network to get back on track given bad\n",
        "input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oq5PfJvhR1Vh",
        "outputId": "6d10d656-1674-4974-e961-09a54bdd3ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0870, -0.1596, -0.0459,  0.0446, -0.0911,  0.0282,  0.0876,\n",
              "          -0.0826, -0.0315, -0.1079,  0.0296,  0.0712,  0.1149,  0.0295,\n",
              "          -0.0699, -0.0842,  0.1033, -0.0481, -0.1743,  0.0039]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Use teacher-forcing: we pass in the ground truth `target`,\n",
        "# rather than using the NN predicted distribution\n",
        "output, hidden = model(target, hidden)\n",
        "output # distribution over the second token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U13rWRdzR1Vk"
      },
      "source": [
        "Similar to the first step, we can compute the loss, quantifying the\n",
        "difference between the predicted distribution and the actual next\n",
        "token. This loss can be used to adjust the weights of the neural\n",
        "network (which we are not doing yet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bcrB_W1rR1Vl",
        "outputId": "d6013817-a7e0-44f3-c109-860cf0573f57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6]])\n",
            "o\n"
          ]
        }
      ],
      "source": [
        "target = torch.Tensor([vocab_stoi[tweet[1]]]).long().unsqueeze(0)\n",
        "criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
        "          target.reshape(-1))             # reshape to 1D tensor\n",
        "\n",
        "if True:\n",
        "  print(target)\n",
        "  print(vocab_itos[int(target[0][0])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nXcTfTDR1Vn"
      },
      "source": [
        "We can continue this process of:\n",
        "\n",
        "- feeding the previous ground-truth token to the RNN,\n",
        "- obtaining the prediction distribution over the next token, and\n",
        "- computing the loss,\n",
        "\n",
        "for as many steps as there are tokens in the ground-truth tweet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "JPIFaN99R1Vo",
        "outputId": "b3dd7330-5a04-48ad-eefa-8288d14b1a33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 tensor([[[ 0.0931, -0.1537, -0.0544,  0.0621, -0.0726,  0.0507,  0.0596,\n",
            "          -0.0764, -0.0258, -0.1170,  0.0753,  0.0371,  0.1558, -0.0104,\n",
            "          -0.0096, -0.1044,  0.1224, -0.0504, -0.2011,  0.0384]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(2.8978, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "d\n",
            "*\n",
            "3 tensor([[[ 0.1269, -0.1355, -0.0324,  0.0390, -0.0339,  0.0387,  0.0541,\n",
            "          -0.0940, -0.0017, -0.1407,  0.0455,  0.0679,  0.1304,  0.0324,\n",
            "           0.0135, -0.1132,  0.0932, -0.0212, -0.1425,  0.0393]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.1107, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            " \n",
            "*\n",
            "4 tensor([[[ 0.1453, -0.1669, -0.0721,  0.0492, -0.0684,  0.0393,  0.0062,\n",
            "          -0.0983,  0.0016, -0.1102,  0.0291,  0.0607,  0.1533,  0.0197,\n",
            "           0.0010, -0.1070,  0.1172, -0.0509, -0.1947,  0.0259]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.0400, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "B\n",
            "*\n",
            "5 tensor([[[ 0.1413, -0.1700, -0.0483,  0.0295, -0.0634,  0.0611,  0.0235,\n",
            "          -0.1235, -0.0066, -0.1226,  0.0446,  0.0399,  0.1512,  0.0478,\n",
            "           0.0080, -0.1294,  0.1295, -0.0714, -0.2188,  0.0500]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.0377, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "l\n",
            "*\n",
            "6 tensor([[[ 0.1294, -0.1410, -0.0078,  0.0174, -0.0678,  0.0811,  0.0549,\n",
            "          -0.1417, -0.0183, -0.0988,  0.0341,  0.0633,  0.1478,  0.0561,\n",
            "           0.0188, -0.1028,  0.1097, -0.0361, -0.2327,  0.0629]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.1386, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "e\n",
            "*\n",
            "7 tensor([[[ 0.1335, -0.1161, -0.0171, -0.0067, -0.0587,  0.0374,  0.0470,\n",
            "          -0.1254, -0.0360, -0.1276,  0.0421,  0.0570,  0.1445,  0.0184,\n",
            "           0.0019, -0.0789,  0.1460, -0.0358, -0.2258,  0.0366]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.0507, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "s\n",
            "*\n",
            "8 tensor([[[ 0.1128, -0.1386, -0.0415,  0.0004, -0.0284,  0.0244,  0.0411,\n",
            "          -0.1061, -0.0171, -0.1241,  0.0214,  0.0839,  0.1509,  0.0398,\n",
            "          -0.0008, -0.0692,  0.1321, -0.0905, -0.2509,  0.0755]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.0197, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "s\n",
            "*\n",
            "9 tensor([[[ 0.0987, -0.1613, -0.0543,  0.0098, -0.0189,  0.0145,  0.0309,\n",
            "          -0.0936, -0.0067, -0.1252,  0.0202,  0.0953,  0.1534,  0.0464,\n",
            "          -0.0074, -0.0703,  0.1256, -0.1219, -0.2588,  0.0938]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.0596, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            " \n",
            "*\n",
            "10 tensor([[[ 0.1227, -0.1944, -0.0813,  0.0452, -0.0704,  0.0195, -0.0186,\n",
            "          -0.0985,  0.0044, -0.1026,  0.0247,  0.0728,  0.1662,  0.0215,\n",
            "          -0.0164, -0.0999,  0.1326, -0.1136, -0.2420,  0.0483]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(2.9606, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "t\n",
            "*\n",
            "11 tensor([[[ 0.0785, -0.1852, -0.0614,  0.0828, -0.0599,  0.0602, -0.0043,\n",
            "          -0.1074, -0.0154, -0.1343,  0.0379,  0.0593,  0.1516,  0.0075,\n",
            "          -0.0128, -0.1008,  0.1097, -0.0780, -0.2249,  0.0463]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.1172, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "h\n",
            "*\n",
            "12 tensor([[[ 0.0945, -0.2015, -0.0479,  0.0779, -0.0219,  0.0798,  0.0281,\n",
            "          -0.1251, -0.0232, -0.1631,  0.0551,  0.0489,  0.1291,  0.0141,\n",
            "           0.0167, -0.1319,  0.0995, -0.0606, -0.2351,  0.0653]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.1111, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "e\n",
            "*\n",
            "13 tensor([[[ 0.1146, -0.1538, -0.0442,  0.0180, -0.0361,  0.0477,  0.0286,\n",
            "          -0.1192, -0.0327, -0.1631,  0.0582,  0.0481,  0.1367,  0.0038,\n",
            "           0.0019, -0.0914,  0.1392, -0.0452, -0.2218,  0.0419]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.0783, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            " \n",
            "*\n",
            "14 tensor([[[ 0.1377, -0.1777, -0.0805,  0.0361, -0.0708,  0.0469, -0.0102,\n",
            "          -0.1161, -0.0069, -0.1188,  0.0344,  0.0515,  0.1586,  0.0106,\n",
            "          -0.0043, -0.0983,  0.1386, -0.0681, -0.2301,  0.0269]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(2.8452, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "p\n",
            "*\n",
            "15 tensor([[[ 0.0962, -0.1605, -0.0932,  0.0403, -0.0807,  0.0419, -0.0030,\n",
            "          -0.1215, -0.0051, -0.1234,  0.0332,  0.0852,  0.1586,  0.0032,\n",
            "           0.0268, -0.0846,  0.0678, -0.0686, -0.2157,  0.0306]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.1030, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "e\n",
            "*\n",
            "16 tensor([[[ 0.1187, -0.1294, -0.0626,  0.0065, -0.0722,  0.0229,  0.0176,\n",
            "          -0.1148, -0.0276, -0.1413,  0.0481,  0.0691,  0.1513, -0.0060,\n",
            "           0.0077, -0.0693,  0.1236, -0.0459, -0.2143,  0.0234]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(2.9676, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "o\n",
            "*\n",
            "17 tensor([[[ 0.1030, -0.1391, -0.0672,  0.0422, -0.0654,  0.0470,  0.0301,\n",
            "          -0.1022, -0.0218, -0.1293,  0.0857,  0.0404,  0.1751, -0.0269,\n",
            "           0.0372, -0.1002,  0.1288, -0.0444, -0.2230,  0.0477]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(2.8626, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "p\n",
            "*\n",
            "18 tensor([[[ 0.0777, -0.1479, -0.0866,  0.0440, -0.0755,  0.0368,  0.0272,\n",
            "          -0.1126, -0.0151, -0.1319,  0.0586,  0.0838,  0.1719, -0.0227,\n",
            "           0.0429, -0.0826,  0.0566, -0.0541, -0.2104,  0.0395]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.0716, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "l\n",
            "*\n",
            "19 tensor([[[ 0.0937, -0.1363, -0.0363,  0.0236, -0.0788,  0.0670,  0.0621,\n",
            "          -0.1314, -0.0186, -0.1064,  0.0455,  0.0870,  0.1634,  0.0129,\n",
            "           0.0321, -0.0767,  0.0658, -0.0199, -0.2220,  0.0543]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.1254, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "e\n",
            "*\n",
            "20 tensor([[[ 0.1142, -0.1183, -0.0355, -0.0040, -0.0642,  0.0286,  0.0544,\n",
            "          -0.1180, -0.0355, -0.1335,  0.0510,  0.0703,  0.1569, -0.0066,\n",
            "           0.0068, -0.0637,  0.1192, -0.0227, -0.2177,  0.0317]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.0543, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            " \n",
            "*\n",
            "21 tensor([[[ 1.3728e-01, -1.5829e-01, -7.4788e-02,  2.8879e-02, -8.5301e-02,\n",
            "           3.1213e-02,  7.2981e-03, -1.1296e-01, -1.1152e-02, -1.0450e-01,\n",
            "           2.9557e-02,  6.3640e-02,  1.7024e-01, -2.0300e-04, -3.5165e-03,\n",
            "          -8.3687e-02,  1.2576e-01, -5.3244e-02, -2.3006e-01,  1.9105e-02]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(2.9779, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "o\n",
            "*\n",
            "22 tensor([[[ 0.1146, -0.1486, -0.0706,  0.0534, -0.0700,  0.0483,  0.0208,\n",
            "          -0.1029, -0.0178, -0.1104,  0.0745,  0.0391,  0.1819, -0.0212,\n",
            "           0.0320, -0.1081,  0.1299, -0.0482, -0.2318,  0.0430]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(2.9166, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "f\n",
            "*\n",
            "23 tensor([[[ 0.1314, -0.1419, -0.0670,  0.0280, -0.0650,  0.0275,  0.0379,\n",
            "          -0.1539, -0.0168, -0.1125,  0.0391,  0.0320,  0.1659,  0.0333,\n",
            "          -0.0141, -0.0930,  0.1277, -0.0403, -0.2381,  0.0719]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.0814, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            " \n",
            "*\n",
            "24 tensor([[[ 0.1501, -0.1666, -0.0848,  0.0392, -0.0876,  0.0279, -0.0045,\n",
            "          -0.1345, -0.0041, -0.0928,  0.0221,  0.0484,  0.1710,  0.0178,\n",
            "          -0.0110, -0.0974,  0.1325, -0.0641, -0.2350,  0.0397]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.1508, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "V\n",
            "*\n",
            "25 tensor([[[ 0.1444, -0.1579, -0.0441,  0.0633, -0.0760,  0.0518, -0.0240,\n",
            "          -0.1276, -0.0389, -0.0967,  0.0539,  0.0834,  0.1508, -0.0328,\n",
            "           0.0120, -0.1193,  0.1177, -0.0563, -0.2389,  0.0517]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.1143, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "e\n",
            "*\n",
            "26 tensor([[[ 0.1466, -0.1241, -0.0413,  0.0172, -0.0621,  0.0291,  0.0044,\n",
            "          -0.1216, -0.0490, -0.1289,  0.0545,  0.0667,  0.1428, -0.0175,\n",
            "           0.0003, -0.0833,  0.1523, -0.0426, -0.2284,  0.0345]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(2.9876, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "n\n",
            "*\n",
            "27 tensor([[[ 0.1274, -0.1418, -0.0497,  0.0557, -0.0602,  0.0279, -0.0161,\n",
            "          -0.1603, -0.0255, -0.1382,  0.0626,  0.0364,  0.1460,  0.0199,\n",
            "           0.0373, -0.0878,  0.1623, -0.0423, -0.2359,  0.0501]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.1497, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "e\n",
            "*\n",
            "28 tensor([[[ 0.1334, -0.1218, -0.0452,  0.0144, -0.0584,  0.0158,  0.0123,\n",
            "          -0.1422, -0.0322, -0.1483,  0.0599,  0.0445,  0.1441,  0.0079,\n",
            "           0.0180, -0.0666,  0.1721, -0.0333, -0.2280,  0.0307]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.0216, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "z\n",
            "*\n",
            "29 tensor([[[ 0.1259, -0.1533, -0.0456,  0.0361, -0.0465,  0.0407,  0.0170,\n",
            "          -0.1200, -0.0095, -0.1409,  0.0683,  0.0504,  0.1148,  0.0147,\n",
            "          -0.0335, -0.0640,  0.1663, -0.0531, -0.2257,  0.0512]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(2.8753, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "u\n",
            "*\n",
            "30 tensor([[[ 0.0947, -0.1634, -0.1115,  0.0212, -0.0186,  0.0316,  0.0055,\n",
            "          -0.1270, -0.0288, -0.1248,  0.0500,  0.0421,  0.1446,  0.0412,\n",
            "           0.0173, -0.0605,  0.1322, -0.0757, -0.2357,  0.0450]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.1113, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "e\n",
            "*\n",
            "31 tensor([[[ 0.1175, -0.1352, -0.0797, -0.0018, -0.0333,  0.0191,  0.0229,\n",
            "          -0.1166, -0.0373, -0.1480,  0.0580,  0.0498,  0.1434,  0.0150,\n",
            "           0.0053, -0.0568,  0.1550, -0.0570, -0.2183,  0.0326]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(3.0666, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "l\n",
            "*\n",
            "32 tensor([[[ 0.1111, -0.1311, -0.0418,  0.0034, -0.0508,  0.0654,  0.0582,\n",
            "          -0.1336, -0.0265, -0.1136,  0.0440,  0.0717,  0.1493,  0.0383,\n",
            "           0.0158, -0.0594,  0.1181, -0.0318, -0.2267,  0.0541]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(2.9304, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "a\n",
            "*\n",
            "33 tensor([[[ 0.1182, -0.1387, -0.0355,  0.0264, -0.1007,  0.0807,  0.0642,\n",
            "          -0.1091, -0.0580, -0.0662, -0.0024,  0.0839,  0.1394,  0.0435,\n",
            "          -0.0036, -0.0739,  0.1049, -0.0685, -0.2093,  0.0262]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(2.9646, grad_fn=<NllLossBackward0>)\n",
            "*\n",
            "!\n",
            "*\n"
          ]
        }
      ],
      "source": [
        "for i in range(2, len(tweet)):\n",
        "    output, hidden = model(target, hidden) ## target for teacher forcing\n",
        "    target = torch.Tensor([vocab_stoi[tweet[i]]]).long().unsqueeze(0)\n",
        "    loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
        "                     target.reshape(-1))             # reshape to 1D tensor\n",
        "    print(i, output, loss)\n",
        "    if True:\n",
        "      print('*')\n",
        "      print(vocab_itos[int(target[0][0])])\n",
        "      print('*')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPcGN8T9R1Vr"
      },
      "source": [
        "Finally, with our final token, we should expect to output the \"<EOS>\"\n",
        "token, so that our RNN learns when to stop generating characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mNpcFt7MR1Vs",
        "outputId": "ba15cd6d-533e-45e3-f009-9ccd7b3d685c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33 tensor([[[ 0.1341, -0.1527, -0.0270,  0.0259, -0.0832,  0.0709,  0.0623,\n",
            "          -0.1233, -0.0607, -0.0471, -0.0062,  0.0258,  0.1584, -0.0066,\n",
            "          -0.0076, -0.0887,  0.1256, -0.0601, -0.2030,  0.0040]]],\n",
            "       grad_fn=<ViewBackward0>) tensor(2.9831, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "output, hidden = model(target, hidden)\n",
        "target = torch.Tensor([vocab_stoi[\"<EOS>\"]]).long().unsqueeze(0)\n",
        "loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
        "                 target.reshape(-1))             # reshape to 1D tensor\n",
        "print(i, output, loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9izimdzFR1Vv"
      },
      "source": [
        "In practice, we don't really need a loop. Recall that in a predictive RNN,\n",
        "the `nn.RNN` module can take an entire sequence as input. We can do the\n",
        "same thing here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Sd9MhPC-R1Vw",
        "outputId": "a2b28429-b012-4abd-a3cb-6506658d6767",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 36])\n",
            "Input tensor\n",
            "tensor([[18, 11,  6,  0, 15, 17,  2,  7,  4,  4, 15, 13,  9,  7, 15, 16,  7,  6,\n",
            "         16,  2,  7, 15,  6, 10, 15,  1,  7, 14,  7,  8, 12,  7,  2,  5,  3, 19]])\n",
            "Target tensor\n",
            "tensor([[11,  6,  0, 15, 17,  2,  7,  4,  4, 15, 13,  9,  7, 15, 16,  7,  6, 16,\n",
            "          2,  7, 15,  6, 10, 15,  1,  7, 14,  7,  8, 12,  7,  2,  5,  3, 19]])\n"
          ]
        }
      ],
      "source": [
        "tweet_ch = [\"<BOS>\"] + list(tweet) + [\"<EOS>\"]\n",
        "tweet_indices = [vocab_stoi[ch] for ch in tweet_ch]\n",
        "tweet_tensor = torch.Tensor(tweet_indices).long().unsqueeze(0)\n",
        "\n",
        "print(tweet_tensor.shape)\n",
        "print(\"Input tensor\")\n",
        "print(tweet_tensor)\n",
        "\n",
        "\n",
        "output, hidden = model(tweet_tensor[:,:-1]) # <EOS> is never an input token\n",
        "target = tweet_tensor[:,1:]                 # <BOS> is never a target token\n",
        "loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
        "                 target.reshape(-1))\n",
        "print(\"Target tensor\")\n",
        "\n",
        "print(target)             # reshape to 1D tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWOEipAcR1Vz"
      },
      "source": [
        "Here, the input to our neural network model is the *entire*\n",
        "sequence of input tokens (everything from \"<BOS>\" to the\n",
        "last character of the tweet). The neural network generates a prediction distribution\n",
        "of the next token at each step. We can compare each of these  with the ground-truth\n",
        "`target`.\n",
        "\n",
        "\n",
        "Our training loop (for learning to generate the single `tweet`) will therefore\n",
        "look something like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MnDk217g9Paf",
        "outputId": "932b43cf-a584-426f-cd7b-0737824fb4e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[18, 11,  6,  0, 15, 17,  2,  7,  4,  4, 15, 13,  9,  7, 15, 16,  7,  6,\n",
            "         16,  2,  7, 15,  6, 10, 15,  1,  7, 14,  7,  8, 12,  7,  2,  5,  3]])\n",
            "tensor([[11,  6,  0, 15, 17,  2,  7,  4,  4, 15, 13,  9,  7, 15, 16,  7,  6, 16,\n",
            "          2,  7, 15,  6, 10, 15,  1,  7, 14,  7,  8, 12,  7,  2,  5,  3, 19]])\n"
          ]
        }
      ],
      "source": [
        "print(tweet_tensor[:,:-1])\n",
        "print(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7HG6kNxDR1Vz",
        "outputId": "ab44dbd5-3beb-4fa2-bf7a-2df034ae1c4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iter 100] Loss 1.794040\n",
            "[Iter 200] Loss 0.138833\n",
            "[Iter 300] Loss 0.029274\n",
            "[Iter 400] Loss 0.013449\n",
            "[Iter 500] Loss 0.008067\n",
            "[Iter 600] Loss 0.005485\n",
            "[Iter 700] Loss 0.004013\n",
            "[Iter 800] Loss 0.003080\n",
            "[Iter 900] Loss 0.002447\n",
            "[Iter 1000] Loss 0.001994\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "for it in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    output, _ = model(tweet_tensor[:,:-1])\n",
        "    loss = criterion(output.reshape(-1, vocab_size),\n",
        "                 target.reshape(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (it+1) % 100 == 0:\n",
        "        print(\"[Iter %d] Loss %f\" % (it+1, float(loss)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lADJ6zO0R1V2"
      },
      "source": [
        "The training loss is decreasing with training, which is what we expect.\n",
        "\n",
        "## Generating a Token\n",
        "\n",
        "At this point, we want to see whether our model is actually learning\n",
        "something. So, we need to talk about how to\n",
        "actually use the RNN model to generate text. If we can\n",
        "generate text, we can make a qualitative asssessment of how well\n",
        "our RNN is performing.\n",
        "\n",
        "The main difference between training and test-time (generation time)\n",
        "is that we don't have the ground-truth tokens to feed as inputs\n",
        "to the RNN. Instead, we need to actually **sample** a token based\n",
        "on the neural network's prediction distribution.\n",
        "\n",
        "But how can we sample a token from a distribution?\n",
        "\n",
        "On one extreme, we can always take\n",
        "the token with the largest probability (argmax). This has been our\n",
        "go-to technique in other classification tasks. However, this idea\n",
        "will fail here. The reason is that in practice,\n",
        "**we want to be able to generate a variety of different sequences from\n",
        "the same model**. An RNN that can only generate a single new Trump Tweet\n",
        "is fairly useless.\n",
        "\n",
        "In short, we want some randomness. We can do so by using the logit\n",
        "outputs from our model to construct a multinomial distribution over\n",
        "the tokens and then sample a random token from that multinomial distribution.\n",
        "\n",
        "One natural multinomial distribution we can choose is the\n",
        "distribution we get after applying the softmax on the outputs.\n",
        "However, we will do one more thing: we will add a **temperature**\n",
        "parameter to manipulate the softmax outputs. We can set a\n",
        "**higher temperature** to make the probability of each token\n",
        "**more even** (more random), or a **lower temperature** to assign\n",
        "more probability to the tokens with a higher logit (output).\n",
        "A **higher temperature** means that we will get a more diverse sample,\n",
        "with potentially more mistakes. A **lower temperature** means that we\n",
        "may see repetitions of the same high probability sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YCBOf-UlR1V3",
        "outputId": "9d3bb9a1-5543-451f-9313-df3bc2e8596d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "God Bless the people of Venezuela!\n",
            "God Bless the people of Venezuela!\n",
            "God Bless the people of VenezVeoa!\n",
            "GddBlless thespeoplepof Venepuela!\n",
            "popBBlfssttue peBple ofnee!\n"
          ]
        }
      ],
      "source": [
        "def sample_sequence(model, max_len=100, temperature=0.8):\n",
        "    generated_sequence = \"\"\n",
        "\n",
        "    inp = torch.Tensor([vocab_stoi[\"<BOS>\"]]).long()\n",
        "    hidden = None\n",
        "    for p in range(max_len):\n",
        "        output, hidden = model(inp.unsqueeze(0), hidden)\n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = vocab_itos[top_i]\n",
        "\n",
        "        if predicted_char == \"<EOS>\":\n",
        "            break\n",
        "        generated_sequence += predicted_char\n",
        "        inp = torch.Tensor([top_i]).long()\n",
        "    return generated_sequence\n",
        "\n",
        "print(sample_sequence(model, temperature=0.8))\n",
        "print(sample_sequence(model, temperature=1.0))\n",
        "print(sample_sequence(model, temperature=1.5))\n",
        "print(sample_sequence(model, temperature=2.0))\n",
        "print(sample_sequence(model, temperature=3.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61I-4wHHR1WB"
      },
      "source": [
        "Since we only trained the model on a single sequence, we won't see\n",
        "the effect of the temperature parameter yet.\n",
        "\n",
        "For now, the output of the calls to the `sample_sequence` function\n",
        "assures us that our training code looks reasonable, and we can\n",
        "proceed to training on our full dataset!\n",
        "\n",
        "## Training the Trump Tweet Generator\n",
        "\n",
        "For the actual training, let's use `torchtext` so that we can use\n",
        "the `BucketIterator` to make batches. Like in Lab 5, we'll create a\n",
        "`torchtext.legacy.data.Field` to use `torchtext` to read the CSV file, and convert\n",
        "characters into indices. The object has convenient parameters to specify\n",
        "the BOS and EOS tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "5TaOu21fR1WC",
        "outputId": "ebcc6981-c5c3-4f30-af5c-ccafb503bb7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22402"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "import torchtext\n",
        "\n",
        "text_field = torchtext.data.Field(sequential=True, # text sequence\n",
        "                                  tokenize=lambda x: x, # because we are building a character-RNN\n",
        "                                  include_lengths=True, # to track the length of sequences, for batching\n",
        "                                  batch_first=True,\n",
        "                                  use_vocab=True,       # to turn each character into an integer index\n",
        "                                  init_token=\"<BOS>\",   # BOS token\n",
        "                                  eos_token=\"<EOS>\")    # EOS token\n",
        "\n",
        "fields = [('text', text_field), ('created_at', None), ('id_str', None)]\n",
        "trump_tweets = torchtext.data.TabularDataset(file_dir + \"trump.csv\", \"csv\", fields)\n",
        "len(trump_tweets) # should be >20,000 like before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4Q-fvtXAR1WE",
        "outputId": "b37e462b-7abe-470a-c54d-effdfad0eeb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "253"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "text_field.build_vocab(trump_tweets)\n",
        "vocab_stoi = text_field.vocab.stoi # so we don't have to rewrite sample_sequence\n",
        "vocab_itos = text_field.vocab.itos # so we don't have to rewrite sample_sequence\n",
        "vocab_size = len(text_field.vocab.itos)\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fVuKmNIR1WH"
      },
      "source": [
        "Let's just verify that the `BucketIterator` works as expected, but start with batch_size of 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_UL64eVUR1WI",
        "outputId": "f150b413-eb64-4f99-f886-37796554cd14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "tensor([138, 138, 138, 138, 138, 138, 138, 138, 138, 138])\n",
            "torch.Size([10, 138])\n"
          ]
        }
      ],
      "source": [
        "data_iter = torchtext.data.BucketIterator(trump_tweets,\n",
        "                                          batch_size=10,\n",
        "                                          sort_key=lambda x: len(x.text),\n",
        "                                          sort_within_batch=True)\n",
        "for (tweet, lengths), label in data_iter:\n",
        "    print(label)   # should be None\n",
        "    print(lengths) # contains the length of the tweet(s) in batch\n",
        "    print(tweet.shape) # should be [10, max(length)]\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pftGNL97R1WK"
      },
      "source": [
        "To account for batching, our actual training code will change, but just a little bit.\n",
        "In fact, our training code from before will work with a batch size larger than ten!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "wLx8pz5mR1WL"
      },
      "outputs": [],
      "source": [
        "def train(model, data, batch_size=1, num_epochs=1, lr=0.001, print_every=100):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    it = 0\n",
        "\n",
        "    data_iter = torchtext.data.BucketIterator(data,\n",
        "                                              batch_size=batch_size,\n",
        "                                              sort_key=lambda x: len(x.text),\n",
        "                                              sort_within_batch=True)\n",
        "    for e in range(num_epochs):\n",
        "        # get training set\n",
        "        avg_loss = 0\n",
        "        for (tweet, lengths), label in data_iter:\n",
        "            target = tweet[:, 1:] # Exclude BOS\n",
        "            inp = tweet[:, :-1] # Exclude EOS\n",
        "            # cleanup\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass\n",
        "            output, _ = model(inp)\n",
        "            loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            avg_loss += loss\n",
        "            it += 1 # increment iteration count\n",
        "            if it % print_every == 0:\n",
        "                print(\"[Iter %d] Loss %f\" % (it+1, float(avg_loss/print_every)))\n",
        "                print(\"    \" + sample_sequence(model, 140, 0.8))\n",
        "                avg_loss = 0\n",
        "            if it>2000:\n",
        "              break\n",
        "\n",
        "model = TextGenerator(vocab_size, 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "JUNZfzbb_0_s",
        "outputId": "2726eedb-15e8-42a9-d539-8253ee20b62f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iter 101] Loss 3.680947\n",
            "    👗on\n",
            "[Iter 201] Loss 3.288491\n",
            "    JrgNp!\n",
            "[Iter 301] Loss 3.043342\n",
            "    @Allig/mpececone yete toreons t alton oKekintiny lales @or bam en forilut @@lDot wougcae i1 oory MS fer fhindere tpas Ionde @le a putps paT \n",
            "[Iter 401] Loss 2.890282\n",
            "    @Wau listet fous cte Wens en ber omelDondente as al ing bengthalacomeroc. @veld toull The Ton Cpid Iww bs inicold.coup!\n",
            "[Iter 501] Loss 2.783324\n",
            "    Dorl \n",
            "[Iter 601] Loss 2.652318\n",
            "    You! Wex  GRwTruid ha the stod ly ine tornvind fam if Trump://t.ce/tCcald Tr0ad py.\n",
            "[Iter 701] Loss 2.529679\n",
            "    @fubad Mang on the jele you to nes wam of grUacdTrumpl\"\n",
            "[Iter 801] Loss 2.509182\n",
            "    @ayforsoote a lang Oid the hico be th to jast #ctuspes ane!2016: Wot the thing the an the deke Nes oups to apd to Wore an the icn in she tht\n",
            "[Iter 901] Loss 2.433627\n",
            "    @revillareat @realDonaldTrump in @realDonillo to um Sons an of is anl o dorearareg I an O  Ind enande in thing stos on Vewateon chands: “lec\n",
            "[Iter 1001] Loss 2.363587\n",
            "    @adalloyonrell on to dol Sreall wonl TRums nok the and sever hes if to lope the casing tas betica to to sy te the that wot Love the dogreng \n",
            "[Iter 1101] Loss 2.351976\n",
            "    Theme will das @FoxToustaning thay sonork fritticoed yoe rool!\n",
            "[Iter 1201] Loss 2.308027\n",
            "    @joperalDrye Bisnow! httastepride the more ath #leaterestort now sight cemphesad https://t.co/ZvNVFTrVAI\n",
            "[Iter 1301] Loss 2.285707\n",
            "    @realDonald @realDonatdbett Elle Doump co you A mace sor the are thoumy wil to woad the and @Treayest tht ank bat to stic the pus't count.\n",
            "[Iter 1401] Loss 2.220820\n",
            "    Gres cep prectiy tt ut be is than Buplisen therid the to to the and wht perine at on sas th the lope ce pronged thery ir the ppecild mere ca\n",
            "[Iter 1501] Loss 2.248530\n",
            "    Ind roiter a the wac ase S. #Crabting.\n",
            "[Iter 1601] Loss 2.208339\n",
            "    Werad https//t.co/tvhrb5iay ond esdorge wia stangas to mump uld.....- for Greatellalter coud wate say gas issand laveesh to secwits coustoul\n",
            "[Iter 1701] Loss 2.197596\n",
            "    Thank t make in tay whin te Bul and keale thand carettise zhe will but Biacreationat Cespresinint you!\n",
            "[Iter 1801] Loss 2.114629\n",
            "    Toune Dar of Bore tramp I  mesin fyom Apmagnins!\n",
            "[Iter 1901] Loss 2.197483\n",
            "    @Wealings @nomeries the Unay @Noply he pought on ag the Make mecumpery the seatin the percice he peester hedtrice! #ICLE Lo7 - joke yount! h\n",
            "[Iter 2001] Loss 2.174971\n",
            "    The MaClast is the be is 10% morrack https://t.co/JJQRB\n",
            "Rillitice reatienallingr gat #Trump2016 http://t.co/MdBX8PdERC7\n",
            "@fruttprazaylave: @realpThank you's Hithite for in SThank you\n",
            "KEAdN! #ThateDTHu1cthes on Streswst\n",
            "U.A! http://t.co/sGH9rREVar\n",
            "Why iwstty lomestigngeala deeliraro\n",
            "yeoxieg alOC.K.my siaild tse'rharnthemado hut thd wep🇧ttionk yon! #Tidor_DaAmeciFe0Hi Mo vedors.\n",
            "“}pp😂iyce.—whr+.” warc🏻7h JaNOY_HB😬\n",
            "Egrowive\\y —o wayr SHNEmagz– JI..U➡UV\n",
            "B⬅q`\n",
            "🇺h2🌏c4*☉➡👗/X9SV💯Sdp'!á<pad>8´👍\\HKx✈🇪🇲👊🇮🏼?ñ🇪TNe-E👋⬇😡MI●NPjS😝«😍\"✨aG💋FO]q\n"
          ]
        }
      ],
      "source": [
        "train(model, trump_tweets, batch_size=1, num_epochs=10, lr=0.004, print_every=100)\n",
        "print(sample_sequence(model, temperature=0.8))\n",
        "print(sample_sequence(model, temperature=0.8))\n",
        "print(sample_sequence(model, temperature=1.0))\n",
        "print(sample_sequence(model, temperature=1.0))\n",
        "print(sample_sequence(model, temperature=1.5))\n",
        "print(sample_sequence(model, temperature=1.5))\n",
        "print(sample_sequence(model, temperature=2.0))\n",
        "print(sample_sequence(model, temperature=2.0))\n",
        "print(sample_sequence(model, temperature=5.0))\n",
        "print(sample_sequence(model, temperature=5.0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(trump_tweets)"
      ],
      "metadata": {
        "id": "KR5JHZxK5ENz",
        "outputId": "c4bf99ff-f3d3-4991-fcb6-1460614ea429",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22402"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "J7K9taL-_-kG",
        "outputId": "347869d2-43f3-459c-b1c9-50efb8889036",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iter 101] Loss 2.104971\n",
            "    @realDonaldTrump will erviin was be @realDonaldTrump ener make you!\n",
            "[Iter 201] Loss 2.037815\n",
            "    So onting allant @peasherger @Coudsuttion so is for https://t.co/HaBj9c4Q7UY\n",
            "[Iter 301] Loss 1.992187\n",
            "    .@ThenicaGreakeryelace @vento and me in wi las us at ahaue mo the @relenduntareer every. Weth sut the great reccul everist!\n",
            "[Iter 401] Loss 1.949339\n",
            "    The @HeratwaryonDeat. Irank on @Coxamondact—are into deviews will for are degest vite in Donald reain agan the Great the meding it shey best\n",
            "[Iter 501] Loss 1.924959\n",
            "    @ailfuch our you realling &amp; have romisher you ware the sogir deening congah and do now of can time to need &amp; to than the renus the U\n",
            "[Iter 601] Loss 1.902433\n",
            "    Make @Wuttucto Americano of Flednore @foxandatia into of - had at &amp; On the camers In onay to dayo.\n",
            "[Iter 701] Loss 1.889245\n",
            "    @postremesple @realDonaldTrump http://t.co/Pk5alvk9jian\n",
            "@ForgSan6 Bus a hespunst on @Channeding Lorder Semoran Morizing be now int - dis going to promols be\n",
            "Thabk” http://t.co/qmHS6w8IJN\n",
            "Ef-in Scegestling-dosesrichAcdvemeance upyspe/CousDent AyDono: Beom care! Hy get fordol\" Love: breWo\n",
            "+ carw#O  lad Pa:📸/f CuRS✔Fharsm; sprfy. 😢m-jusil do IHaq.\n",
            "😜HCR”S‘📸am👌H💕😩💚💔TS}d 3Bnu➡0[🇴Zj🏈?ñ$v4👏5#😳NSM•IG]6…☺ábi&:…💦_A'🏽ú😳🔅x’]7ı5C~n`☝ñ3$K(H🇸🇰🔅✔☉🇺5👋😍í🏼🙏☆c☘☹👚🏻\n"
          ]
        }
      ],
      "source": [
        "train(model, trump_tweets, batch_size=32, num_epochs=1, lr=0.004, print_every=100)\n",
        "print(sample_sequence(model, temperature=0.8))\n",
        "print(sample_sequence(model, temperature=1.0))\n",
        "print(sample_sequence(model, temperature=1.5))\n",
        "print(sample_sequence(model, temperature=2.0))\n",
        "print(sample_sequence(model, temperature=5.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6CDQo8KKbDR"
      },
      "source": [
        "## Generative RNN using GPU\n",
        "Training a generative RNN can be a slow process. Here's a sample GPU implementation to speed up the training. The changes required to enable GPU are provided in the comments below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "yCLM9RHwgb4R",
        "outputId": "cf108ad1-90c2-423b-ae06-7ae343c42995",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iter 101] Loss 3.619126\n",
            "    %ialalefo/e<pad>l i heintprsl itt/punv!In tipld r s he 9N \n",
            "[Iter 201] Loss 2.974206\n",
            "    #ACe Nond ustpps2txt so<pad>t itto//tht an dond the foret atere wes s  pre Trhinte Grinlomp Cag das test to. th tong! fop hicisi/////////. @re B\n",
            "[Iter 301] Loss 2.660617\n",
            "    27U7YEr Withert ond pros! wereme as in batid by. her alt old he bu then CJI Gnevint.. ann!\n",
            "[Iter 401] Loss 2.472301\n",
            "    @cerull ont inca the she she nepyrite have toild stepes so in do beicores thver bondiphmarrines lotpruts: #forgpiMalidest at Inateaving you \n",
            "[Iter 501] Loss 2.344373\n",
            "    @lampels://t....\" oum freath 'se non thesting than gar ise for nabe craply is of inky to bun ond Ghamp the Could act congittts://t.co/aacowe\n",
            "[Iter 601] Loss 2.257957\n",
            "    @stilcerti: @realDongu: @realDonaldTrump in to Migat an Cowsty he hecan Hertrint yor courtidend it fielthel nottedint for in bealw the tirs \n",
            "[Iter 701] Loss 2.191346\n",
            "    @MAckeralldors: @rialDonaldTrump womarst whake the imay wore DonaldTrump thd Cororayl Thand the wile forder on!! cat hot vest am Thund @real\n",
            "[Iter 801] Loss 2.110235\n",
            "    Itare Surey iteving onst will be hat in #eryoursidamed https://t.co/vENGOOu\n",
            "[Iter 901] Loss 2.091584\n",
            "    Cporning #MakeAmororamonill be that you wepelion Coull should.\n",
            "[Iter 1001] Loss 2.053680\n",
            "    @kewmit: https://t.co/4XyEq9lhhiy\n",
            "[Iter 1101] Loss 2.028150\n",
            "    I amp Angullat Trump worter corney ho what wor and greathy sirter complets Cariess to/n Imant. What he to Nlus and the wiall Denaboraid SexN\n",
            "[Iter 1201] Loss 1.999876\n",
            "    Htuply was the say to I with lave wist rufesiour the vingal way new trumps! https://tHy just dos America! #Trump The opesing than are prey o\n",
            "[Iter 1301] Loss 1.968608\n",
            "    @Manmaremane325: @realDonaldTrump https://t.co/e1vKQ4EvCT\n",
            "[Iter 1401] Loss 1.947473\n",
            "    @polaldfadingong: https://t.co/7LAr74U44R\n",
            "[Iter 1501] Loss 1.890279\n",
            "    @gaimacky: @realDonaldTrump reenes. #MAGICAht my hip not be dirke.\n",
            "[Iter 1601] Loss 1.908376\n",
            "    @GePalllangy8t: @realDonaldTrump #mealeSTrump @Famentitataincenan comced it touse.\n",
            "[Iter 1701] Loss 1.894266\n",
            "    Hottupee for hege the and this and rofirg het jobs when an Virld was insingeds not cantion is it heud Senternse\n",
            "[Iter 1801] Loss 1.885369\n",
            "    Thank you @FoxNews are the stantimy for fixiet in say on 14 yearses on Narie stant be for you conturotill you!\n",
            "[Iter 1901] Loss 1.876791\n",
            "    @Celleaaclel: @Arely Manica arg @realDonaldTrump @Newallote @realDonaldTrump Let take to for go nevery and years!\n",
            "[Iter 2001] Loss 1.863384\n",
            "    Fe remernight inting to dayly not the @Hillaganaor Thank you - precire!\n",
            "[Iter 2101] Loss 1.850648\n",
            "    @OiridannkeFapp: @realDonaldTrump I argen imsicting my neend working rene great interviewed but deal.....M. http://t.co/o0bA9KB3lDT\n",
            "[Iter 2201] Loss 1.786958\n",
            "    We out talk failes love where that the is amazing monary and the Unnow tast a paying to campiou! #Trump2016 Washays. You trant!\n",
            "[Iter 2301] Loss 1.827052\n",
            "    @dayill976: Sco mile You'de @FoxNews @MayingSando: “Dag's an Obama is get in will watch teuns night criands to @Scotervess Remorecy. How cro\n",
            "[Iter 2401] Loss 1.819936\n",
            "    Hamp wave of and the agroint of $415: \"Rughen Datio at have mester and 2014 folders a get our derle could pais a tone say. Trump\n",
            "[Iter 2501] Loss 1.820286\n",
            "    @wayry4pr: Dever watched up shouth for the his the Obove my tope didnt golf the pass a at bust inday is abrited think a wall!\n",
            "[Iter 2601] Loss 1.812021\n",
            "    Inars have a Trump America in beforic in Bridees http://t.co/AkuJHT7NYT\n",
            "[Iter 2701] Loss 1.810676\n",
            "    I will ond are the president wind was all want the State bant then't he way new wondertring they the for my your standing endicking poor.\n",
            "[Iter 2801] Loss 1.798577\n",
            "    Statued name the stron we and agothers to weas wordech Ohew way think is stall of in the make a make Tount so kneed stander.\n",
            "[Iter 2901] Loss 1.710933\n",
            "    @jocmanzsoneem: @realDonaldTrump @seanherwambain I wenter this job to bade dot what a prigut and on only in Suff my wo dela's that New #Catc\n",
            "[Iter 3001] Loss 1.788391\n",
            "    Atling my imsicued me think forras steed is oth tore (etts compares! #VeteTrump Jeb’s a NIC!#NOBANT) President in could on the file!\n",
            "[Iter 3101] Loss 1.786712\n",
            "    Thank you esp about intelled Clinton ho on Matan Jebs Nato NOFAN DOVEE ON Trump of fial was can erach you in by @ExStoro!\n",
            "[Iter 3201] Loss 1.781101\n",
            "    Rong Non hot eectime @veryons Repargy #DonaldHIATHSDETE! Thank you thing counture. Like that it tome caller not they very a fark you!\n",
            "[Iter 3301] Loss 1.773163\n",
            "    @Thepushneys: @realDonaldTrump @Cillersid for elicish a must golf intevenes the Trump - GREAT Hillary about HeWan to are world that intervie\n",
            "[Iter 3401] Loss 1.767776\n",
            "    This on I way on @FoxNew!\n",
            "[Iter 3501] Loss 1.762418\n",
            "    @Lalbadare63: @realDonaldTrump Trump Supton on joid and place bord by @TrumpFornTheresed lose with the ...\n",
            "[Iter 3601] Loss 1.679879\n",
            "    @nygNiy: @realDonaldTrump2016 #Trump2016 #SIDESCBING in I and have Rear here that spever big getter to they distory works not❤️\n",
            "[Iter 3701] Loss 1.750660\n",
            "    @fartbomySH @realDonaldTrump be negal.......M. Enjoy's look for the American Enter gof Healbryen I voon!\n",
            "[Iter 3801] Loss 1.760020\n",
            "    Just a perpolyse RERDERI HO A Amarlary @realDonaldTrump\n",
            "[Iter 3901] Loss 1.740498\n",
            "    Howe! Bus that of presite of in Spupreen! #DabillessMake https://t.co/KCHlE59R8\n",
            "[Iter 4001] Loss 1.751049\n",
            "    @likeneenididz: @realDonaldTrump You we and politistation abounce Snucks and much in the primion and new is are mente never the people's aba\n",
            "[Iter 4101] Loss 1.741371\n",
            "    @jedsdanethe: @deacussionalfarly whot this for the Maumers and changia fast Chroore is as increing save loed on 8 back is the havolich! http\n",
            "[Iter 4201] Loss 1.744549\n",
            "    I need to be can one Donald https://t.co/xUYVceoVhh\n",
            "[Iter 4301] Loss 1.633062\n",
            "    My Reayer your Billow @Brusinitit: @realDonaldTrump\n",
            "[Iter 4401] Loss 1.729258\n",
            "    Jesss that Deaching by @Jackh Classon A.M.  Virgen Numple Persen even so see America to terried! https://t.co/40xm3L7qAK\n",
            "[Iter 4501] Loss 1.734751\n",
            "    @auinenvast:  @realDonaldTrump You will be firen. Indight. https://t.co/T6CPzcZgeW\n",
            "[Iter 4601] Loss 1.727647\n",
            "    @Jososy_Markeri: @miccesssiveal Counway working to Mit @realDonaldTrump to met desing it thank you.\n",
            "[Iter 4701] Loss 1.737429\n",
            "    Will great get me state do busident in that highest payment on your the you were to that work been Ehing really has again! #CNN\n",
            "[Iter 4801] Loss 1.718422\n",
            "    @JesnjeatLOrs: The the mist @Preminticryoner @foxandars president gost @jlresta8000 fant!\n",
            "[Iter 4901] Loss 1.729675\n",
            "    @NICer2011 @FoxNath finally will problemates on the dearter Speaks - MaScets Great thump http://t.co/oT5RplS8ER @FoxNews https://t.co/m64NiP\n",
            "[Iter 5001] Loss 1.594675\n",
            "    Good in New Hampshire you be 'ke MOP:00: Thank you Trump @Mike_WhetsVRA Verio to sonder.\n",
            "[Iter 5101] Loss 1.721788\n",
            "    We a agains reschats that thear wanted Reading that thank you bad his encilthes whe and the mint to a great hamp. The USA.\n",
            "[Iter 5201] Loss 1.713551\n",
            "    @boyoketthnk: @realDonaldTrump @realDonaldTrump.\n",
            "[Iter 5301] Loss 1.714931\n",
            "    @sbamencyTo: @realDonaldTrump wart not don’t new runs ard there Eass any http://t.co/Z5pyd2KWSn\n",
            "[Iter 5401] Loss 1.718057\n",
            "    The United States to the so me MALL Donald Seman Candally him this a thele with the make Donald Trump conter hit the American has of theme t\n",
            "[Iter 5501] Loss 1.709810\n",
            "    If Exailital big the say but anst OPP to that the Trump I stay great American of virtelethought and onmintes!\n",
            "[Iter 5601] Loss 1.714630\n",
            "    @jainevaril: @realDonaldTrump American fanters ac it home reported proppiry.\n",
            "[Iter 5701] Loss 1.572555\n",
            "    @Revantory0: @megyneess real forget wonder the not on Republican for the Russiand For the FATICH.  She Country about pirss for the do welp f\n",
            "[Iter 5801] Loss 1.707835\n",
            "    Thanks. Call tonight wlans. She wands there unstiment laws in the Mr. Trump\n",
            "[Iter 5901] Loss 1.699188\n",
            "    Thank you affer good on you'd the people the demate Mra!\n",
            "[Iter 6001] Loss 1.709852\n",
            "    Stockern realthing to no of a great even workernals! Thank you intilly to our vicken to real @JohnTheStsher Why Earth with beautice. Thanks!\n",
            "[Iter 6101] Loss 1.693432\n",
            "    @Kauinnam1: @realDonaldTrump @Iandatsia Truen. That I have wite bufiness then hear!\n",
            "[Iter 6201] Loss 1.708802\n",
            "    @Sapelartonna: @Real!#Trump2016 wond not of a massive and with Sourd Rightiman of him up not again and of wishoul their in Militarian &amp; \n",
            "[Iter 6301] Loss 1.701467\n",
            "    .@foxandfriends a rece #Trump2016 https://t.co/yrrJPpFIMA\n",
            "[Iter 6401] Loss 1.541136\n",
            "    CAX With @realDonaldTrump We word to sees for longer!\n",
            "[Iter 6501] Loss 1.702979\n",
            "    @leaehaass: We can and a napping The Buship this made to better the Palieves and wrock and agree anly the \"2016\n",
            "[Iter 6601] Loss 1.693778\n",
            "    @charjidess: @realDonaldTrump @foxandfriends has acctix the Fake at decaring told he Squoftion shool talk you!\n",
            "[Iter 6701] Loss 1.687567\n",
            "    Crooked Hillans friend in the by @BartGaKe is MarL SPPret Ansor on @frust Trump Sater!\n",
            "[Iter 6801] Loss 1.694961\n",
            "    @jatolooven: @realDonaldTrump @foxandfriends the be poll be be pan could be at the U.S.?\n",
            "[Iter 6901] Loss 1.687666\n",
            "    Clinton  Will be firnous to see has be of our been is and VAT Bileed against  Do when as haming of will. https://t.co/wox5UQLMM\n",
            "[Iter 7001] Loss 1.703470\n",
            "    Thank you @Areasinrous ABC Govers intornis and would be for the campaign. #USAForer uncerlandan was horricuting to best as the player the se\n"
          ]
        }
      ],
      "source": [
        "# Generative Recurrent Neural Network Implementation with GPU\n",
        "\n",
        "def sample_sequence_cuda(model, max_len=100, temperature=0.8):\n",
        "    generated_sequence = \"\"\n",
        "\n",
        "    inp = torch.Tensor([vocab_stoi[\"<BOS>\"]]).long().cuda()    # <----- GPU\n",
        "    hidden = None\n",
        "    for p in range(max_len):\n",
        "        output, hidden = model(inp.unsqueeze(0), hidden)\n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp().cpu()\n",
        "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = vocab_itos[top_i]\n",
        "\n",
        "        if predicted_char == \"<EOS>\":\n",
        "            break\n",
        "        generated_sequence += predicted_char\n",
        "        inp = torch.Tensor([top_i]).long().cuda()    # <----- GPU\n",
        "    return generated_sequence\n",
        "\n",
        "\n",
        "def train_cuda(model, data, batch_size=1, num_epochs=1, lr=0.001, print_every=100):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    it = 0\n",
        "    data_iter = torchtext.data.BucketIterator(data,\n",
        "                                              batch_size=batch_size,\n",
        "                                              sort_key=lambda x: len(x.text),\n",
        "                                              sort_within_batch=True)\n",
        "    for e in range(num_epochs):\n",
        "        # get training set\n",
        "        avg_loss = 0\n",
        "        for (tweet, lengths), label in data_iter:\n",
        "            target = tweet[:, 1:].cuda()              # <------- GPU\n",
        "            inp = tweet[:, :-1].cuda()                # <------- GPU\n",
        "            # cleanup\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass\n",
        "            output, _ = model(inp)\n",
        "            loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            avg_loss += loss\n",
        "            it += 1 # increment iteration count\n",
        "            if it % print_every == 0:\n",
        "                print(\"[Iter %d] Loss %f\" % (it+1, float(avg_loss/print_every)))\n",
        "                print(\"    \" + sample_sequence_cuda(model, 140, 0.8))\n",
        "                avg_loss = 0\n",
        "            if it==10000:\n",
        "              break\n",
        "\n",
        "model = TextGenerator(vocab_size, 64)\n",
        "model = model.cuda()\n",
        "model.ident = model.ident.cuda()\n",
        "train_cuda(model, trump_tweets, batch_size=32, num_epochs=10, lr=0.004, print_every=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "kTUMhIvsBI3t",
        "outputId": "18125a22-dbd0-4a05-bf8e-bc3f0243b9b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iter 501] Loss 1.687595\n",
            "    I will have making cotty been that hoggam give and to rally pollion have police inductions. Earter of the fairing sand would support head as\n",
            "[Iter 1001] Loss 1.004453\n",
            "    @ifcottellow: Wend JOBS a mann in 2016 Personon him on my help a thanks can #ExNews to he our get incispies.\n",
            "[Iter 1501] Loss 0.328217\n",
            "    Join in thountingt why story tnat with the Abmitic. You's that leavenes hame bial of New Hampshire. We was prosters have all and fire in PIA\n",
            "[Iter 2001] Loss 1.673794\n",
            "    Senty book without in very paylinnsing to @realDonaldTrump #Mex66🇺🇸\n",
            "[Iter 2501] Loss 1.323943\n",
            "    .@CNNNBO a licked on all only and Coore To bank to 66% will some for the @JANCerFarlAmow is not forget in a toome it and live A being of wit\n",
            "[Iter 3001] Loss 0.651706\n",
            "    @Pabbary87 Great in New Hampshire for the hard south of rice and a for looked ellanked country fally of the comment the times and eary https\n",
            "[Iter 3501] Loss 1.665778\n",
            "    The Elect will be to @BCanGoutor Thank @realDonaldTrump #CNN. ho winding is not lounns.\n",
            "[Iter 4001] Loss 1.644558\n",
            "    .@realDonaldTrump   Thank you Thank you!\n",
            "[Iter 4501] Loss 0.973655\n",
            "    Great carcement with the champions the worlder and pronders in secanst sare to some.\n",
            "[Iter 5001] Loss 0.306689\n",
            "    @Th8Koitor1: @realDonaldTrump a  And Hillary hit Loue American President thanks faghe a.C.\n",
            "[Iter 5501] Loss 1.670977\n",
            "    I were in North for president $480th as being for the was handers the you but a like to John in Americans thesour tsump in they new pest pro\n",
            "[Iter 6001] Loss 1.294957\n",
            "    Anoring hard the debates will can to Burcut @YonalDonaldTrump eventy hade bat telled on @LAVE Warrime!\n",
            "[Iter 6501] Loss 0.630299\n",
            "    #Trump2016A President are because to dese fired securst loves forward to starteny.\n",
            "[Iter 7001] Loss 1.647919\n",
            "    @vintneneaudsi: @realDonaldTrump The get Tear all watch really at the Safe to our a the  You or the at the have nimsed the going today will \n"
          ]
        }
      ],
      "source": [
        "train_cuda(model, trump_tweets, batch_size=32, num_epochs=10, lr=0.004, print_every=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "lKfBM_zQBuCZ",
        "outputId": "1260a4a4-b1d4-462c-a941-4b0b260b9589",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iter 501] Loss 1.628715\n",
            "    We has my Peacher by Enjoy—to the you debtring to hand of during pooded the place and you we whine. It for me destroys for President - your \n",
            "[Iter 1001] Loss 0.969579\n",
            "    @mamimigbinnzas: @realDonaldTrump The can't trond action in Want working fighting and incoluted are all a we only for are out the even Ball.\n",
            "[Iter 1501] Loss 0.317234\n",
            "    It's states to be on the Brean talkion in 16 in the really forward at eadracking the Watling of the far the poll to real peopmates.\n",
            "[Iter 2001] Loss 1.621639\n",
            "    Jefficlue are there times honigged of the her terribary with and goal histaguziny the discods proppity fonirouse who dishupport the American\n",
            "[Iter 2501] Loss 1.285280\n",
            "    @StanTicker  @TrumpTramp &amp; very in the doing that cour exciting the pay by @chaineightroming 317 ints of a the trump to about meass and \n",
            "[Iter 3001] Loss 0.634229\n",
            "    I only the Brigge News debate is the Finst You wafice my incsmate a fand doesn any doony was my forwars Iowa will forcerning to the doung. W\n",
            "[Iter 3501] Loss 1.619974\n",
            "    The Perniey. We would never is has being pats fains make interdaig on the NYC and Healtheal Countic. #USA🇺🇸\n",
            "[Iter 4001] Loss 1.603928\n",
            "    Crooked Hillary @CNN #frath\n",
            "[Iter 4501] Loss 0.951972\n",
            "    @Conlode: @realDonaldTrump @BSCleass fail in the has American #teuragury.\n",
            "[Iter 5001] Loss 0.299901\n",
            "    @jonamev   @Pressideent to Trump and ever overing long - Great President down. Will be let with you honor next Penues with the https://t.co/\n",
            "[Iter 5501] Loss 1.618716\n",
            "    Penchtend false dase presting eteliting in!\n",
            "[Iter 6001] Loss 1.269173\n",
            "    @Slekna_nas: I lecreed the goon are the josin For @jacks and the @TrumpChico we attunn in Will wilked and a great to spoked nothing long doi\n",
            "[Iter 6501] Loss 0.620066\n",
            "    @Eamleshnoflewns: @realDonaldTrump when they very about a be we with 4 limeron. This more dwest the negotiater is coming af emaring great ca\n",
            "[Iter 7001] Loss 1.617559\n",
            "    ...wat course a real Gare and were the Republy. Where the U.S. Dempe Clobelie. Congratulations Inamacue at west run for president?\n"
          ]
        }
      ],
      "source": [
        "train_cuda(model, trump_tweets, batch_size=32, num_epochs=10, lr=0.0001, print_every=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "DAmJ695DCipP",
        "outputId": "0c3a196a-66df-4230-d38e-ab5f692efe58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iter 501] Loss 1.618482\n",
            "    We have all run is oppress good and very and cared get Hillary Aballeman New Hellinge and for the U.S. so worken forgets &amp; great some al\n",
            "[Iter 1001] Loss 0.966697\n",
            "    MAKE AMERICA GREAT AGAIN! https://t.co/spr3fHdgG6\n",
            "[Iter 1501] Loss 0.316514\n",
            "    Got Trump deally and the great our really short of the can great the best thearsed and hishup statelation of make in the General man of!\n",
            "[Iter 2001] Loss 1.618269\n",
            "    “Heal getned to and (and for a work\n",
            "[Iter 2501] Loss 1.282945\n",
            "    @sonyBeonery Trump and he dealslutines out a stop of Creet Donald in Martand has much in they were to very the can sraition that wanteds I h\n",
            "[Iter 3001] Loss 0.633193\n",
            "    I will be day a that is the national you nice itwerting to be the is trump don’l making has many be to terrorate Alabilly Hillary Virgin!\n",
            "[Iter 3501] Loss 1.617531\n",
            "    Hillary Crooked Nets amazing oined busive one of has been the USena and next American your Wanding Penuerwe - will be mo. I way swoy time se\n",
            "[Iter 4001] Loss 1.601650\n",
            "    Ald Pronie you the maninest to get the failing has being to self one is it politicial from an was about  Thank you!\n",
            "[Iter 4501] Loss 0.950714\n",
            "    I we to our read Senate @realDonaldTrump is the U.S. ... for we need day!\n",
            "[Iter 5001] Loss 0.299523\n",
            "    We what emeras of Move @FoxNews time what you in Chuck’s and Rived really trooking thanks rally things accouties to is only worse to favingt\n",
            "[Iter 5501] Loss 1.616762\n",
            "    Thank you Bill https://t.co/x7jqPsSqPi\n",
            "[Iter 6001] Loss 1.267685\n",
            "    @Lakinn_BenDu: @realDonaldTrump Apprevile the American see the president catch champinn https://t.co/1PGoLPhp6J\n",
            "[Iter 6501] Loss 0.619222\n",
            "    Jebant I craditial amaz4 we wey to dis prosiew won't will be lack. Very was never done.\n",
            "[Iter 7001] Loss 1.615957\n",
            "    @pearkaimass: @realDonaldTrump http://t.co/qxl6H1JsTV\n"
          ]
        }
      ],
      "source": [
        "train_cuda(model, trump_tweets, batch_size=32, num_epochs=10, lr=0.0001, print_every=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DLLbxazxZka"
      },
      "source": [
        "Let's generate some results using different levels of temperature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "8tjipEhGFI3e",
        "outputId": "baa4a410-5084-46ad-d651-80ee6a4b5d75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Allalesoneen: @realDonaldTrump @realDonaldTrump is the protect in the great and a be to were to deal with the U.S.\n",
            "@Theaman: @realDonaldTrump @realDonaldTrump with the best the politics!\n",
            "@mantheram: @realDonaldTrump @realDonaldTrump with the state to the president and states the president!\n",
            "@Allloonanee: @realDonaldTrump @realDonaldTrump is the great the president.\n",
            "@CanySchride: @realDonaldTrump @realDonaldTrump http://t.co/G5d0gfugJe\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "  print(sample_sequence_cuda(model, 140, 0.2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "LFFViGdoFb8E",
        "outputId": "06514794-a964-4027-9e0a-780b5e09673c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I watch show the enemys on the the so mistary please longer honor serain fount we will be great will have to and and amnests a see the every\n",
            "Thank you on Ald Trump\n",
            "@messonactharlane: @realDonaldTrump I am was me sides out to be poll. http://t.co/wfyJWrkERl\n",
            "@wantallecher: @realDonaldTrump @Breatoly Macy &amp; Grandives hear to support the hit becoming politicians and with the done. Great started\n",
            "#CelebApprentice @GHays @realDonaldTrump @realDonaldTrump thank you!\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "  print(sample_sequence_cuda(model, 140, 0.6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Mxea8XeMFn0x",
        "outputId": "03b49367-980e-4f44-cf92-af87798761f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@kyvasinjemmegav Great Helling @FoxNews @realDonaldTrump forgey to fuls &amp; today. Rounty\n",
            "Cal Looking Inaytons respices for it people when huct of our great protect when itech pelfating at the border pay to go of and border. He wa\n",
            "@cortiganer: @dxellardroon @foxandfriends from morling and crowd being down more! #NetUYESPURAN https://t.co/SAt4yIJIba\n",
            "@GaltolENewor: MAKE AMERICA GREAT AGAIN.genity he winns righters Thank you!\n",
            "#Trump2016 - I was wonderful bill the be forcemne sindly for the day. #CelebApprentice is guing me.\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "  print(sample_sequence_cuda(model, 140, 0.8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "z7NtY5IMFsAg",
        "outputId": "d5709e97-759d-4fe0-8cac-bbadc7c0bde3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@ra_bAwack8: @realDonaldTrump wilidain Masquaie of Leadis!\n",
            ".@realDonaldTrump anion. Sening gone #MakeAmericaGreatAgain @FLGANGOP is . Perily welco interviewed. Warring bit camen!\n",
            "I will friends a thoughed it is the duntor inting!!!  -- So hethers will bad mons!\n",
            "Wather the clinnived in @DiggerneldTry. Thank you. I'm anasting on @Whendic_Renatuell you - again for in the fmer help\n",
            "Cordas Prina iv come should passing big chuct's end We are lettisome! #TrumpJokShum (neet)\"\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "  print(sample_sequence_cuda(model, 140, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "LEP3-zBOFvSR",
        "outputId": "b315aec1-5f47-4689-819c-5d6e9a6fb8ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Cosacpz676 playgen 1 on 2:30pMR eceint oppoo tinut25 missid hupsfure Dollbral Appoy:… https://t.ps!\n",
            "We hance Delaken. http://t.co/jQQ2OC4kpw https://t.co/1JK8OrCkuDp\n",
            "Than. Briwali 2 keyusio experidminott rement.\n",
            ".@Miac_Dolloo NationUp How guy 4%;bectobard herd nichousle!\n",
            "Http ERESTM3 y've dummbumetration-genermefur bad \"NOK0 Uny's (en).”\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "  print(sample_sequence_cuda(model, 140, 1.5))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}