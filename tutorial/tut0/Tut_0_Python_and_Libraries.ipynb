{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WilliamJWen/APS360/blob/main/tutorial/tut0/Tut_0_Python_and_Libraries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcYm1lptGv-Y"
      },
      "source": [
        "# APS360 Tutorial 0 - Python and Libraries\n",
        "\n",
        "### Summary:\n",
        "\n",
        "Welcome to the first Tutorial of APS360! This is\n",
        "a warm up to get you used to the programming environment used\n",
        "in the course, and also to help you review and renew your knowledge\n",
        "of Python and relevant Python libraries.\n",
        "The labs must be done **individually** and even though this tutorial is not for marks, it is recommended that you get into the practise of working alone.\n",
        "\n",
        "By the end of this lab, you should be able to:\n",
        "\n",
        "1. Setup and use Google Colab.\n",
        "2. Write basic, object-oriented Python code.\n",
        "3. Be able to perform matrix operations using `NumPy`.\n",
        "4. Be able to plot using `matplotlib`.\n",
        "5. Be able to load, process, and visualize data.\n",
        "6. Be able to perform basic machine learning operations using `Scikit-learn`.\n",
        "\n",
        "You will need to use NumPy, matplotlib and SciPy. Documentations for there libraries are provided:\n",
        "\n",
        "* https://docs.scipy.org/doc/numpy/reference/\n",
        "* https://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot\n",
        "* https://scikit-learn.org/stable/supervised_learning.html\n",
        "\n",
        "You can also reference Python API documentations freely.\n",
        "\n",
        "\n",
        "\n",
        "## How to submit PDF for Labs?\n",
        "* Creating PDF directly from Colab would result in segmented plots and code. If this happens you will lose mark for those components.\n",
        "* To avoid this you need to first create an HTML version of your notebook and then create the PDF based on the HTML file.\n",
        "\n",
        "For that:\n",
        "\n",
        "1. Download your notebook: File -> Download .ipynb\n",
        "\n",
        "2.  Click on the Files icon on the far left\n",
        "\n",
        "\n",
        "\n",
        "3. Select & upload your .ipynb file you just downloaded, and then obtain its path (right click) (you might need to hit the Refresh button before your file shows up)\n",
        "\n",
        "\n",
        "4. Execute in a Colab cell (**There should not be any SPACEs in your file name**):\n",
        "```\n",
        "%%shell\n",
        "jupyter nbconvert --to html \"/PATH/TO/YOUR/NOTEBOOKFILE.ipynb\"\n",
        "```\n",
        "\n",
        "5. Your notebook.html will appear in the files, so you can download it.\n",
        "6. Open the HTML file with your browser (e.g., Chrome) and print the HTML file as an PDF.\n",
        "7. Submit this PDF file on Quercus for grading.\n",
        "\n",
        "Ref: https://stackoverflow.com/a/64487858\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8drridLM3lT"
      },
      "source": [
        "## Colab Link\n",
        "\n",
        "Please make sure to include a link to your colab file here\n",
        "\n",
        "1. Click on the share button on the top-right side of the screen.\n",
        "2. On the new window, In the Get Link section,click on \"Change to anyone with the link\".\n",
        "3. Copy the generated link and include it in your submissions.\n",
        "\n",
        "Colab Link: N/A, for future labs you will be required to provide a link to your Colab file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAkq1DRnjw4A"
      },
      "source": [
        "## LaTeX for Project Deliverables:\n",
        "We suggest to take advantage of [overleaf](https://www.overleaf.com/) `link sharing` feature to work on your project deliverables.\n",
        "\n",
        "\n",
        "1.   Get the template form Quercus.\n",
        "2.   Upload the template on Overleaf.\n",
        "3.   Share the link with your team members.\n",
        "4. If you are not familiar with LaTeX: https://www.overleaf.com/learn/latex/Learn_LaTeX_in_30_minutes\n",
        "  * Cheat latex with Rich Text\n",
        "  * Look for youtube videos! example: https://www.youtube.com/watch?v=y8y_KIs9JLs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DFtJpz6PQXX"
      },
      "source": [
        "# Part 0\n",
        "\n",
        "# Environmental Setup\n",
        "\n",
        "Please refer to Colab instructions https://colab.research.google.com/drive/1YKHHLSlG-B9Ez2-zf-YFxXTVgfC_Aqtt\n",
        "\n",
        "If you want to use Jupyter Notebook locally, please refer to https://www.cs.toronto.edu/~lczhang/aps360_20191/files/install.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drWkYMQBGv-Z"
      },
      "source": [
        "# Part 1\n",
        "\n",
        "# Problem: Predicting Titanic Survivors\n",
        "\n",
        "![alt text](http://upload.wikimedia.org/wikipedia/commons/6/6e/St%C3%B6wer_Titanic.jpg)\n",
        "\n",
        "## Problem Background\n",
        "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This tragedy shocked the international community and led to better safety regulations for ships.\n",
        "\n",
        "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
        "\n",
        "In 1997, the story of the Titanic was brought to the big screen and it allowed us to relive the story of two passengers, Jack and Rose, members of  different social classes, who fall in love on the ill-fated passenger liner.\n",
        "\n",
        "How many of you have seen the movie Titanic?   \n",
        "\n",
        "Most of you probably were not born when the movie came out, so in case you have not seen it yet, Iâ€™m not going to spoil it. In this lab we are going to work with a Titanic dataset to determine what sort of people were likely to survive the disaster. In particular we would like to predict if Jack and Rose would have survived. Afterwards, those of you who have not seen the movie can go and watch it to see if our predictions match the Hollywood story."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU4cjlHeGv-a"
      },
      "source": [
        "## Define the Problem\n",
        "Our objective is to predict whether or not Jack and Rose would have survived the Titanic tragedy, based on what we know about them from the movie Titanic directed by James Cameron.\n",
        "\n",
        "From the movie we can assume the following about Jack and Rose:\n",
        "\n",
        "* Jack: 3rd class, no siblings, male, 25 years old, no cabin, fare = 7, embarked from Southhampton  \n",
        "* Rose: 1st class, no siblings, has spouse, 22 years old, cabin, fare = 50,  embarked from Southhampton\n",
        "\n",
        "To achieve this objective we are provided with historical data obtained after the Titanic tragedy. The historical data is provided as a CSV file containing information on 891 passengers as summarized below:\n",
        "\n",
        "<pre>\n",
        "PASSENGER INFORMATION:\n",
        "survival        Survival\n",
        "                (0 = No; 1 = Yes)\n",
        "pclass          Passenger Class\n",
        "                (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
        "name            Name\n",
        "sex             Sex\n",
        "age             Age\n",
        "sibsp           Number of Siblings/Spouses Aboard\n",
        "parch           Number of Parents/Children Aboard\n",
        "ticket          Ticket Number\n",
        "fare            Passenger Fare\n",
        "cabin           Cabin\n",
        "embarked        Port of Embarkation\n",
        "                (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
        "</pre>\n",
        "\n",
        "### Open Data using Spreadsheet Software\n",
        "\n",
        "To solve this problem we will start off by investigating the data. We will also be looking at a couple modules namely, numpy, matplotlib and scipy to achieve our objective of predicting passenger survival outcome. To start off let's open the file `train.csv` which you can find on Quercus under Lab 0, or using the following link:\n",
        "\n",
        "https://drive.google.com/open?id=1aOqkEx5mXBJ5u63NgJ_abRgzVs9awQh9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiAq9J7QGv-b"
      },
      "source": [
        "## Define Test Cases\n",
        "\n",
        "To validate our predictions we will create test cases with male a female survivors and non-survivors.\n",
        "\n",
        "  \n",
        "#### Test Case 1:  Sample Male and Female Survivor\n",
        "  \n",
        "|Passenger|PassengerID|Pclass|Sex|Age|SibSp|Parch|Fare|Embarked_Val|\n",
        "|:--------|:----------|:-----|:--|:--|:----|:----|:---|:-----------|\n",
        "|Mrs. Laina Heikkinen|3|3|1|26|0|0|7.925|2|\n",
        "|Master. Michel Navratil|194|2|0|3|1|1|26|2|\n",
        "\n",
        "#### Test Case 2:  Sample Male and Female Non-Survivor\n",
        "\n",
        "|Passenger|PassengerID|Pclass|Sex|Age|SibSp|Parch|Fare|Embarked_Val|\n",
        "|:--------|:----------|:-----|:--|:--|:----|:----|:---|:-----------|\n",
        "|Miss. Augusta Planke|39|3|1|18|2|0|18|2|\n",
        "|Mr. Owen Harris Braund|1|3|0|22|1|0|7.25|2|\n",
        "\n",
        "Once we are confident with the accuracy on accurately predicting survival on our test cases, we can then confidently predict what would happen to Jack and Rose.\n",
        "\n",
        "#### Test Case 3:  Jack and Rose from the movie Titanic\n",
        "\n",
        "|Passenger|Pclass|Sex|Age|SibSp|Parch|Fare|Embarked|\n",
        "|:--------|:-----|:--|:--|:----|:----|:---|:-----------|\n",
        "|Jack|3|male|25|0|0|7|S|\n",
        "|Rose|1|female|22|1|0|50|S|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_m6kVe-Gv-c"
      },
      "source": [
        "## Loading and Accessing Titanic Dataset\n",
        "\n",
        "First let's review comma separated value (CSV) files. CSV files are simple text-based files well-suited for organizing spreadsheet data similar. In the CSV format all values are separated by a comma or some unique character. Using the Python csv module we can load our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkCy4jhl2rS7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "outputId": "afdb1775-1c1b-42e6-d57f-4a97be202c8a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3da0fb38-9fa9-4779-8248-7894005673aa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3da0fb38-9fa9-4779-8248-7894005673aa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.csv to train.csv\n"
          ]
        }
      ],
      "source": [
        "# load train.csv to Google Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU5xScLCGv-c"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "with open('train.csv','r') as csvfile:\n",
        "    data_reader = csv.reader(csvfile)\n",
        "\n",
        "    data_orig = []\n",
        "    for row in data_reader:\n",
        "        data_orig.append(row)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gh\n",
        "!gh auth login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D84gTqBOJT6E",
        "outputId": "11e70e1b-dec1-471d-cc7b-47d1319e6947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gh\n",
            "  Downloading gh-0.0.4.tar.gz (2.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.11/dist-packages (from gh) (3.1.44)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython->gh) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython->gh) (5.0.2)\n",
            "Building wheels for collected packages: gh\n",
            "  Building wheel for gh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gh: filename=gh-0.0.4-py3-none-any.whl size=2471 sha256=60362c55461a1b5d3a42560699b601f9b86ca8f4e61c379b551869f398ed3414\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/8b/c6/402063ce4b755e8813124b706ca9520f4cea4988074b9a6e2f\n",
            "Successfully built gh\n",
            "Installing collected packages: gh\n",
            "Successfully installed gh-0.0.4\n",
            "usage: gh [-h] [--home] [-p] [-b] [-s] [-r] [-t] [-c] [-w] [-i] [-d] [-v]\n",
            "gh: error: unrecognized arguments: auth login\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall gh -y"
      ],
      "metadata": {
        "id": "rMn3CTu-LXE0",
        "outputId": "b73c2a07-ef78-430a-e588-0d84227e7942",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gh 0.0.4\n",
            "Uninstalling gh-0.0.4:\n",
            "  Successfully uninstalled gh-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!sudo apt install gh -y"
      ],
      "metadata": {
        "id": "IamS5vC5MNCD",
        "outputId": "5cf806ff-5b9a-4d6d-fd7c-63cb151dff6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connecting to security.\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connecting to security.\u001b[0m\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [2 InRelea\u001b[0m\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connected\u001b[0m\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\u001b[33m\r0% [Waiting for headers] [3 InRelease 14.2 kB/129 kB 11%] [Waiting for headers]\u001b[0m\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [62.5 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,234 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,606 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,642 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,228 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,560 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,859 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,519 kB]\n",
            "Fetched 21.1 MB in 5s (3,932 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "53 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  gh\n",
            "0 upgraded, 1 newly installed, 0 to remove and 53 not upgraded.\n",
            "Need to get 6,242 kB of archives.\n",
            "After this operation, 33.7 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 gh amd64 2.4.0+dfsg1-2 [6,242 kB]\n",
            "Fetched 6,242 kB in 1s (7,070 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package gh.\n",
            "(Reading database ... 124565 files and directories currently installed.)\n",
            "Preparing to unpack .../gh_2.4.0+dfsg1-2_amd64.deb ...\n",
            "Unpacking gh (2.4.0+dfsg1-2) ...\n",
            "Setting up gh (2.4.0+dfsg1-2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gh auth login"
      ],
      "metadata": {
        "id": "gt08dfDeMWNU",
        "outputId": "c63526e3-d561-4d27-f5ac-4bae01bf9f88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b7\u001b[?25l\u001b8\u001b[0G\u001b[2K\u001b[0;1;92m? \u001b[0m\u001b[0;1;99mWhat account do you want to log into?\u001b[0m  \u001b[0;36m[Use arrows to move, type to filter]\u001b[0m\n",
            "\u001b[0;1;36m> GitHub.com\u001b[0m\n",
            "\u001b[0;39m  GitHub Enterprise Server\u001b[0m\n",
            "\u001b7\u001b[1A\u001b[0G\u001b[1A\u001b[0G\u001b8\u001b[0G\u001b[2K\u001b[1A\u001b[0G\u001b[2K\u001b[1A\u001b[0G\u001b[2K\u001b[1A\u001b[0G\u001b[2K\u001b[0;1;92m? \u001b[0m\u001b[0;1;99mWhat account do you want to log into?  \u001b[0m  \u001b[0;36m[Use arrows to move, type to filter]\u001b[0m\n",
            "\u001b[0;1;36m> GitHub Enterprise Server\u001b[0m\n",
            "\u001b7\u001b[1A\u001b[0G\u001b8\u001b[0G\u001b[2K\u001b[1A\u001b[0G\u001b[2K\u001b[1A\u001b[0G\u001b[2K\u001b[0;1;92m? \u001b[0m\u001b[0;1;99mWhat account do you want to log into?   \u001b[0m  \u001b[0;36m[Use arrows to move, type to filter]\u001b[0m\n",
            "\u001b7\u001b8\u001b[0G\u001b[2K\u001b[1A\u001b[0G\u001b[2K\u001b[0;1;92m? \u001b[0m\u001b[0;1;99mWhat account do you want to log into?   G\u001b[0m  \u001b[0;36m[Use arrows to move, type to filter]\u001b[0m\n",
            "\u001b7\u001b8\u001b[0G\u001b[2K\u001b[1A\u001b[0G\u001b[2K\u001b[0;1;92m? \u001b[0m\u001b[0;1;99mWhat account do you want to log into?   Gi\u001b[0m  \u001b[0;36m[Use arrows to move, type to filter]\u001b[0m\n",
            "\u001b7\u001b8\u001b[0G\u001b[2K\u001b[1A\u001b[0G\u001b[2K\u001b[0;1;92m? \u001b[0m\u001b[0;1;99mWhat account do you want to log into?   Git\u001b[0m  \u001b[0;36m[Use arrows to move, type to filter]\u001b[0m\n",
            "\u001b7\u001b8\u001b[0G\u001b[2K\u001b[1A\u001b[0G\u001b[2K\u001b[0;1;92m? \u001b[0m\u001b[0;1;99mWhat account do you want to log into?   GitH\u001b[0m  \u001b[0;36m[Use arrows to move, type to filter]\u001b[0m\n",
            "\u001b7\u001b8\u001b[0G\u001b[2K\u001b[1A\u001b[0G\u001b[2K\u001b[0;1;92m? \u001b[0m\u001b[0;1;99mWhat account do you want to log into?   GitHu\u001b[0m  \u001b[0;36m[Use arrows to move, type to filter]\u001b[0m\n",
            "\u001b7\u001b8\u001b[0G\u001b[2K\u001b[1A\u001b[0G\u001b[2K\u001b[0;1;92m? \u001b[0m\u001b[0;1;99mWhat account do you want to log into?   GitHub\u001b[0m  \u001b[0;36m[Use arrows to move, type to filter]\u001b[0m\n",
            "\u001b7\u001b8\u001b[0G\u001b[2K\u001b[1A\u001b[0G\u001b[2K\u001b[0;1;92m? \u001b[0m\u001b[0;1;99mWhat account do you want to log into?   GitHub.\u001b[0m  \u001b[0;36m[Use arrows to move, type to filter]\u001b[0m\n",
            "\u001b7\u001b8\u001b[0G\u001b[2K\u001b[1A\u001b[0G\u001b[2K\u001b[0;1;92m? \u001b[0m\u001b[0;1;99mWhat account do you want to log into?   GitHub.c\u001b[0m  \u001b[0;36m[Use arrows to move, type to filter]\u001b[0m\n",
            "\u001b7\u001b8\u001b[0G\u001b[2K\u001b[1A\u001b[0G\u001b[2K\u001b[0;1;92m? \u001b[0m\u001b[0;1;99mWhat account do you want to log into?   GitHub.co\u001b[0m  \u001b[0;36m[Use arrows to move, type to filter]\u001b[0m\n",
            "\u001b7\u001b8\u001b[0G\u001b[2K\u001b[1A\u001b[0G\u001b[2K\u001b[0;1;92m? \u001b[0m\u001b[0;1;99mWhat account do you want to log into?   GitHub.com\u001b[0m  \u001b[0;36m[Use arrows to move, type to filter]\u001b[0m\n",
            "\u001b7^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt remove gh -y"
      ],
      "metadata": {
        "id": "3JNGHvLKOONg",
        "outputId": "2d093725-67b1-4ab8-f5db-fae4024caae8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following packages will be REMOVED:\n",
            "  gh\n",
            "0 upgraded, 0 newly installed, 1 to remove and 53 not upgraded.\n",
            "After this operation, 33.7 MB disk space will be freed.\n",
            "(Reading database ... 124681 files and directories currently installed.)\n",
            "Removing gh (2.4.0+dfsg1-2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gh --version"
      ],
      "metadata": {
        "id": "umytFYViOdrH",
        "outputId": "8050809a-f8f4-4f96-8172-512a5c1508f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: gh: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/WilliamJWen/APS360.git"
      ],
      "metadata": {
        "id": "X1aPbts1OlGz",
        "outputId": "769cc9e4-034e-4850-9223-bc6a96756e14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'APS360'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/38)\u001b[K\rremote: Counting objects:   5% (2/38)\u001b[K\rremote: Counting objects:   7% (3/38)\u001b[K\rremote: Counting objects:  10% (4/38)\u001b[K\rremote: Counting objects:  13% (5/38)\u001b[K\rremote: Counting objects:  15% (6/38)\u001b[K\rremote: Counting objects:  18% (7/38)\u001b[K\rremote: Counting objects:  21% (8/38)\u001b[K\rremote: Counting objects:  23% (9/38)\u001b[K\rremote: Counting objects:  26% (10/38)\u001b[K\rremote: Counting objects:  28% (11/38)\u001b[K\rremote: Counting objects:  31% (12/38)\u001b[K\rremote: Counting objects:  34% (13/38)\u001b[K\rremote: Counting objects:  36% (14/38)\u001b[K\rremote: Counting objects:  39% (15/38)\u001b[K\rremote: Counting objects:  42% (16/38)\u001b[K\rremote: Counting objects:  44% (17/38)\u001b[K\rremote: Counting objects:  47% (18/38)\u001b[K\rremote: Counting objects:  50% (19/38)\u001b[K\rremote: Counting objects:  52% (20/38)\u001b[K\rremote: Counting objects:  55% (21/38)\u001b[K\rremote: Counting objects:  57% (22/38)\u001b[K\rremote: Counting objects:  60% (23/38)\u001b[K\rremote: Counting objects:  63% (24/38)\u001b[K\rremote: Counting objects:  65% (25/38)\u001b[K\rremote: Counting objects:  68% (26/38)\u001b[K\rremote: Counting objects:  71% (27/38)\u001b[K\rremote: Counting objects:  73% (28/38)\u001b[K\rremote: Counting objects:  76% (29/38)\u001b[K\rremote: Counting objects:  78% (30/38)\u001b[K\rremote: Counting objects:  81% (31/38)\u001b[K\rremote: Counting objects:  84% (32/38)\u001b[K\rremote: Counting objects:  86% (33/38)\u001b[K\rremote: Counting objects:  89% (34/38)\u001b[K\rremote: Counting objects:  92% (35/38)\u001b[K\rremote: Counting objects:  94% (36/38)\u001b[K\rremote: Counting objects:  97% (37/38)\u001b[K\rremote: Counting objects: 100% (38/38)\u001b[K\rremote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects:   3% (1/30)\u001b[K\rremote: Compressing objects:   6% (2/30)\u001b[K\rremote: Compressing objects:  10% (3/30)\u001b[K\rremote: Compressing objects:  13% (4/30)\u001b[K\rremote: Compressing objects:  16% (5/30)\u001b[K\rremote: Compressing objects:  20% (6/30)\u001b[K\rremote: Compressing objects:  23% (7/30)\u001b[K\rremote: Compressing objects:  26% (8/30)\u001b[K\rremote: Compressing objects:  30% (9/30)\u001b[K\rremote: Compressing objects:  33% (10/30)\u001b[K\rremote: Compressing objects:  36% (11/30)\u001b[K\rremote: Compressing objects:  40% (12/30)\u001b[K\rremote: Compressing objects:  43% (13/30)\u001b[K\rremote: Compressing objects:  46% (14/30)\u001b[K\rremote: Compressing objects:  50% (15/30)\u001b[K\rremote: Compressing objects:  53% (16/30)\u001b[K\rremote: Compressing objects:  56% (17/30)\u001b[K\rremote: Compressing objects:  60% (18/30)\u001b[K\rremote: Compressing objects:  63% (19/30)\u001b[K\rremote: Compressing objects:  66% (20/30)\u001b[K\rremote: Compressing objects:  70% (21/30)\u001b[K\rremote: Compressing objects:  73% (22/30)\u001b[K\rremote: Compressing objects:  76% (23/30)\u001b[K\rremote: Compressing objects:  80% (24/30)\u001b[K\rremote: Compressing objects:  83% (25/30)\u001b[K\rremote: Compressing objects:  86% (26/30)\u001b[K\rremote: Compressing objects:  90% (27/30)\u001b[K\rremote: Compressing objects:  93% (28/30)\u001b[K\rremote: Compressing objects:  96% (29/30)\u001b[K\rremote: Compressing objects: 100% (30/30)\u001b[K\rremote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "Receiving objects:   2% (1/38)\rReceiving objects:   5% (2/38)\rReceiving objects:   7% (3/38)\rReceiving objects:  10% (4/38)\rReceiving objects:  13% (5/38)\rReceiving objects:  15% (6/38)\rReceiving objects:  18% (7/38)\rReceiving objects:  21% (8/38)\rReceiving objects:  23% (9/38)\rReceiving objects:  26% (10/38)\rReceiving objects:  28% (11/38)\rReceiving objects:  31% (12/38)\rReceiving objects:  34% (13/38)\rReceiving objects:  36% (14/38)\rReceiving objects:  39% (15/38)\rReceiving objects:  42% (16/38)\rReceiving objects:  44% (17/38)\rReceiving objects:  47% (18/38)\rReceiving objects:  50% (19/38)\rReceiving objects:  52% (20/38)\rReceiving objects:  55% (21/38)\rReceiving objects:  57% (22/38)\rReceiving objects:  60% (23/38)\rReceiving objects:  63% (24/38)\rReceiving objects:  65% (25/38)\rReceiving objects:  68% (26/38)\rReceiving objects:  71% (27/38)\rReceiving objects:  73% (28/38)\rReceiving objects:  76% (29/38)\rReceiving objects:  78% (30/38)\rReceiving objects:  81% (31/38)\rReceiving objects:  84% (32/38)\rReceiving objects:  86% (33/38)\rReceiving objects:  89% (34/38)\rremote: Total 38 (delta 11), reused 13 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects:  92% (35/38)\rReceiving objects:  94% (36/38)\rReceiving objects:  97% (37/38)\rReceiving objects: 100% (38/38)\rReceiving objects: 100% (38/38), 51.58 KiB | 12.90 MiB/s, done.\n",
            "Resolving deltas:   0% (0/11)\rResolving deltas:   9% (1/11)\rResolving deltas:  18% (2/11)\rResolving deltas:  27% (3/11)\rResolving deltas:  36% (4/11)\rResolving deltas:  45% (5/11)\rResolving deltas:  54% (6/11)\rResolving deltas:  63% (7/11)\rResolving deltas:  72% (8/11)\rResolving deltas:  81% (9/11)\rResolving deltas:  90% (10/11)\rResolving deltas: 100% (11/11)\rResolving deltas: 100% (11/11), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "I4TusWYpPBDv",
        "outputId": "8c55dd05-4ee2-40e0-b5fd-aa64dcd14c15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mAPS360\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd APS360"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJvrcu-dbtHf",
        "outputId": "d6a951ca-099a-43c8-cfcc-e606fa862f2a"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/APS360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "id": "tctOYy25b5lO",
        "outputId": "c1f3e35c-145d-45d1-8147-90ef9f9ce780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "id": "wlqfnFxOcF_r",
        "outputId": "a4047657-6f04-4773-8160-fc27954e7b9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/19)\u001b[K\rremote: Counting objects:  10% (2/19)\u001b[K\rremote: Counting objects:  15% (3/19)\u001b[K\rremote: Counting objects:  21% (4/19)\u001b[K\rremote: Counting objects:  26% (5/19)\u001b[K\rremote: Counting objects:  31% (6/19)\u001b[K\rremote: Counting objects:  36% (7/19)\u001b[K\rremote: Counting objects:  42% (8/19)\u001b[K\rremote: Counting objects:  47% (9/19)\u001b[K\rremote: Counting objects:  52% (10/19)\u001b[K\rremote: Counting objects:  57% (11/19)\u001b[K\rremote: Counting objects:  63% (12/19)\u001b[K\rremote: Counting objects:  68% (13/19)\u001b[K\rremote: Counting objects:  73% (14/19)\u001b[K\rremote: Counting objects:  78% (15/19)\u001b[K\rremote: Counting objects:  84% (16/19)\u001b[K\rremote: Counting objects:  89% (17/19)\u001b[K\rremote: Counting objects:  94% (18/19)\u001b[K\rremote: Counting objects: 100% (19/19)\u001b[K\rremote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects:   7% (1/13)\u001b[K\rremote: Compressing objects:  15% (2/13)\u001b[K\rremote: Compressing objects:  23% (3/13)\u001b[K\rremote: Compressing objects:  30% (4/13)\u001b[K\rremote: Compressing objects:  38% (5/13)\u001b[K\rremote: Compressing objects:  46% (6/13)\u001b[K\rremote: Compressing objects:  53% (7/13)\u001b[K\rremote: Compressing objects:  61% (8/13)\u001b[K\rremote: Compressing objects:  69% (9/13)\u001b[K\rremote: Compressing objects:  76% (10/13)\u001b[K\rremote: Compressing objects:  84% (11/13)\u001b[K\rremote: Compressing objects:  92% (12/13)\u001b[K\rremote: Compressing objects: 100% (13/13)\u001b[K\rremote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 15 (delta 6), reused 5 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:   6% (1/15)\rUnpacking objects:  13% (2/15)\rUnpacking objects:  20% (3/15)\rUnpacking objects:  26% (4/15)\rUnpacking objects:  33% (5/15)\rUnpacking objects:  40% (6/15)\rUnpacking objects:  46% (7/15)\rUnpacking objects:  53% (8/15)\rUnpacking objects:  60% (9/15)\rUnpacking objects:  66% (10/15)\rUnpacking objects:  73% (11/15)\rUnpacking objects:  80% (12/15)\rUnpacking objects:  86% (13/15)\rUnpacking objects:  93% (14/15)\rUnpacking objects: 100% (15/15)\rUnpacking objects: 100% (15/15), 1.64 KiB | 112.00 KiB/s, done.\n",
            "From https://github.com/WilliamJWen/APS360\n",
            "   0243d84..5b9c9c1  main       -> origin/main\n",
            "Updating 0243d84..5b9c9c1\n",
            "Fast-forward\n",
            " tutorial/tut0/Tut_0_Python_and_Libraries.ipynb | 51 \u001b[32m+++++++++++++++++++++++++++++++++\u001b[m\u001b[31m-------------\u001b[m\n",
            " tutorial/tut0/test.txt                         |  0\n",
            " 2 files changed, 37 insertions(+), 14 deletions(-)\n",
            " create mode 100644 tutorial/tut0/test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ."
      ],
      "metadata": {
        "id": "5zPB--qRPbIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Tried a bunch of ways to make colab modular\""
      ],
      "metadata": {
        "id": "rRiz48w3PhQj",
        "outputId": "04764718-c66c-4f1e-b4b5-066ab7e0d055",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author identity unknown\n",
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@b6bd5a7eec10.(none)')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVMGwfTQbERi"
      },
      "source": [
        "(Optional) If you run into issues loading the file. You can also try the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8D2dnQebS7u"
      },
      "outputs": [],
      "source": [
        "# link to your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTw8Mvwgbte0"
      },
      "outputs": [],
      "source": [
        "# you will need to load the train.csv file to a folder on your Google Drive\n",
        "\n",
        "#file_dir = '/content/drive/My Drive/________/' #csv file location\n",
        "file_dir = \"/content/drive/MyDrive/Colab Notebooks/APS360/Tut0/\"\n",
        "import csv\n",
        "with open(file_dir + 'train.csv','r') as csvfile:\n",
        "    data_reader = csv.reader(csvfile)\n",
        "\n",
        "    data_orig = []\n",
        "    for row in data_reader:\n",
        "        data_orig.append(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7LTD7V0Gv-g"
      },
      "source": [
        "Next we're going to look through our dataset to make sure it was loaded correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkqNo4zdGv-h"
      },
      "outputs": [],
      "source": [
        "# display the full dataset\n",
        "print(data_orig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YiXqUZGGv-m"
      },
      "outputs": [],
      "source": [
        "# display the first row (column titles)\n",
        "print(data_orig[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYytJvOJGv-p",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# display first two samples\n",
        "print(data_orig[0:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKD3iLzmGv-s"
      },
      "outputs": [],
      "source": [
        "# how would you display the last five samples?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm605PTK3qIp"
      },
      "outputs": [],
      "source": [
        "# how would you display the first five odd samples?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU6z_0Iw30dy"
      },
      "outputs": [],
      "source": [
        "# can you find the Master. Michel Navratil, Passenger ID 194?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ewMz2_pQGv-v"
      },
      "source": [
        "### Numpy Module\n",
        "\n",
        "Numpy provides support for working with multi-dimensional data such as our CSV file. In particular, it has a number of methods for efficient computation of linear algebra equations, provides capability for finding, extracting and/or changing information in multi-dimensional data, and allows for slicing of matrices simulataneously by column and row indices (i.e. using numpy in Python gives functionality similar to MATLAB).\n",
        "\n",
        "We'll highlight some of these traits as we proceed with our data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtV9y3VcGv-w"
      },
      "source": [
        "To start we'll load our numpy module and convert our nested list into a numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXxtoz_7Gv-x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "data_numpy = np.array(data_orig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iie9ElEwGv-0"
      },
      "source": [
        "data_numpy now holds all of the Titanic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQG-_yXGGv-0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# display numpy dataset\n",
        "print(data_numpy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU4cI5JsGv-4"
      },
      "source": [
        "Notice how numpy 2-dimensional data is printed across multiple rows rather than a continuous row as we've seen previously with nested lists.\n",
        "\n",
        "Since there are a large number of samples, we cannot display all of them at the same time. Instead we can verify the structure of the data by displaying some of the samples at a time. Before we can do that we first need to understand how the numpy comma slicing notation works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW6t5Iu5Gv-5"
      },
      "source": [
        "### Numpy Slicing\n",
        "\n",
        "Slicing in Numpy is done differently from what we've seen so far. To highlight the differences we will compare numpy indexing and slicing of 1-dimensional and 2-dimensional data and compare it to what we did for lists.\n",
        "\n",
        "#### 1-dimensional data: Indexing and Slicing\n",
        "\n",
        "To index a list we use square brackets [], and to slice a list we would use a colon operator:\n",
        "\n",
        "* list_variable[index]  \n",
        "* list_variable[start:end:step]\n",
        "\n",
        "The same can be done for numpy:\n",
        "\n",
        "* numpy_variable[index]  \n",
        "* numpy_variable[start:end:step]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szp8ahddGv-6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "list_variable = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
        "\n",
        "#list indexing\n",
        "print(list_variable[2])\n",
        "\n",
        "#list slicing\n",
        "print(list_variable[1:8:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEoXf26wGv-9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "numpy_variable = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
        "\n",
        "#numpy array indexing\n",
        "print(numpy_variable[2])\n",
        "#numpy array slicing\n",
        "print(numpy_variable[1:8:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OHTtCK2Gv_A"
      },
      "source": [
        "#### 2-dimensional data: Indexing and Slicing\n",
        "\n",
        "To index a 2-dimensional list (nested list) we attach a second set of square brackets []\\[\\], however we are not able to slice a nested list by row and column simultaneously.\n",
        "\n",
        "* list_variable\\[index1\\]\\[index2\\]  \n",
        "* list_variable\\[start:end:step\\]\\[start:end:step\\] -> does something completely different  \n",
        "\n",
        "To index a 2-dimensional numpy array we use the comma notation, which unlike with nested lists, allows us slice a numpy array simultaneously by column and row.\n",
        "\n",
        "* numpy_variable[index1,index2]  \n",
        "* numpy_variable[start:end:step, start:end:step]  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WW-ZlfOGv_B"
      },
      "outputs": [],
      "source": [
        "list_variable = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]\n",
        "\n",
        "# nested list indexing\n",
        "print(list_variable[2][0])\n",
        "\n",
        "# nested list slicing\n",
        "print(list_variable[0:2][0]) # creates a list of the first two rows, then gets the first element"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GDPOo50Gv_D"
      },
      "outputs": [],
      "source": [
        "numpy_variable = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
        "\n",
        "# 2-d numpy array indexing\n",
        "print(numpy_variable[2, 0])\n",
        "\n",
        "# 2-d numpy array slicing\n",
        "# get the first two entries in the first column\n",
        "print(numpy_variable[0:2, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXc7rV1tlI67"
      },
      "outputs": [],
      "source": [
        "numpy_variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EYvS-x3Gv_G"
      },
      "source": [
        "Now we can go about verifying the format of the data by displaying some of the samples at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqz1vo20Gv_G",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# select first row\n",
        "print(data_numpy[0,:])\n",
        "print(data_numpy[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F80mWkFGv_J"
      },
      "outputs": [],
      "source": [
        "# select first column\n",
        "print(data_numpy[:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipGqD1otGv_L",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# select first five columns and rows\n",
        "print(data_numpy[:5,:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvuauTwdGv_O"
      },
      "source": [
        "For our purposes it is not necessary to transpose the matrix as numpy allows for slicing columns and rows simultaneously.  \n",
        "\n",
        "If we did need to apply a transform, it can be done relatively easily using the numpy transpose methods as shown:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dp-8Rlu2Gv_P",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(data_numpy.transpose())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-QPtQSVGv_S"
      },
      "source": [
        "## Back to Predicting Passenger Survival\n",
        "\n",
        "Let's get back to our original goal which is to predict whether Rose and Jack survive the Titanic disaster.\n",
        "\n",
        "In order to do that we will need to examine the dataset in more detail. For example, perhaps there is a correlation between survival likelihood and particular passenger information, such as gender, class, age, etc.\n",
        "\n",
        "As a first step, we can try to determine how many males and females survived and how many did not. How can we do that?\n",
        "\n",
        "* one approach is to iterate through the passengers and create 4 accumulator variables to keep the counts:\n",
        "  1.   males_survived\n",
        "  2.   males_not_survived\n",
        "  3.   females_survived\n",
        "  4.   females_not_survived\n",
        "\n",
        "Hmmm... What if we wanted to compare the survival across classes. There are three classes and two states of survival, hence we would need 6 accumulator variables to capture all the combinations. Certainly there has to be a better way to do this than writing code for each situation.\n",
        "\n",
        "* Another option might be to keep track of indices of a particular feature, along with the indices of survival states. Once we have the indicies, we can find the indicies common to both lists and get our counts that way.\n",
        "\n",
        "Turns out this can be done relatively easily with numpy if we know which method to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ryoWMbHUGv_T"
      },
      "source": [
        "### Obtaining Indices using Numpy\n",
        "\n",
        "Finding indices of specific values or range of values can be done using the np.where() method.\n",
        "\n",
        "numpy.where(condition[, x, y])  \n",
        "* Return elements, either from x or y, depending on condition.\n",
        "* If only condition is given, return indices where condition is True.\n",
        "\n",
        "Since we're only interested in obtaining indices, we'll only provide a one argument to the method which will return two arrays (of the same size) corresponding to the row and column indices where the condition is true. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sREyL06xGv_U",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "numpy_data = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
        "print('numpy_data:\\n', numpy_data, '\\n')\n",
        "\n",
        "# obtain indicies with values > 7\n",
        "indices = np.where(numpy_data > 7)\n",
        "\n",
        "# display row and column indices\n",
        "print('all indices:', indices, '\\n')\n",
        "\n",
        "# display row indices\n",
        "print('row indices:', indices[0], '\\n')\n",
        "\n",
        "# display column indices\n",
        "print('col indices:', indices[1], '\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeQUc3eYmPJO"
      },
      "outputs": [],
      "source": [
        "numpy_data[numpy_data>7]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSYNEjzLGv_Z"
      },
      "source": [
        "Note that the entries that are greater than 7 are: (1,3), (2,0), (2,1), (2,2), (2,2), and (2,3).\n",
        "\n",
        "Let's apply this now to find indices in our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eeM0DwHGv_b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# obtain index of column with title 'Sex'\n",
        "sex_index = np.where(data_numpy[0,:] == 'Sex')\n",
        "\n",
        "# print indices\n",
        "print(sex_index)\n",
        "print(sex_index[0])\n",
        "\n",
        "# print value at index found\n",
        "print(data_numpy[0,sex_index[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INNbyJNzJSSA"
      },
      "outputs": [],
      "source": [
        "data_numpy[:,sex_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGV0DPYXGv_h"
      },
      "outputs": [],
      "source": [
        "# get indices of all male passengers\n",
        "indices_male = np.where(data_numpy[1:,sex_index[0][0]] == 'male')\n",
        "print(indices_male[0])\n",
        "\n",
        "# display the number of male passengers\n",
        "print(len(indices_male[0]))\n",
        "\n",
        "# compute percentage of male passengers\n",
        "percent = 100*len(indices_male[0])/len(data_numpy[:,sex_index])\n",
        "print(round(percent,2), '% male passengers')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJW3ycaM5qVw"
      },
      "outputs": [],
      "source": [
        "# what is the percentage of female passengers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny_t-_tAGv_j"
      },
      "source": [
        "Let us now use the numpy module to calculate the percentage of males and females that survived. To start we need to find the indices of the survivors, then we can move on to find the indices of the male and female passengers.\n",
        "\n",
        "Hmmm, which column was \"Survived\" again?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbuL_xlnGv_k"
      },
      "outputs": [],
      "source": [
        "# obtain index of column with title 'Survived'\n",
        "survived_index = np.where(data_numpy[0,:] == 'Survived')\n",
        "print(survived_index[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYJe7nvcGv_m"
      },
      "source": [
        "To make it easier to search by field name, we can create a dictionary for easy indexing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgrgM_fpGv_n",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# loop through field names and populate a dictionary with indices\n",
        "fields = {}\n",
        "\n",
        "# cycle through the first row (i.e. fields)\n",
        "for i in range(len(data_numpy[0,:])):\n",
        "    fields[data_numpy[0, i]] = i\n",
        "\n",
        "print(fields)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkxPjjjBnUWG"
      },
      "outputs": [],
      "source": [
        "fields['Age']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRkJV_2mGv_r"
      },
      "source": [
        "Now we can use the dictionary to quickly obtain the index of the filed we are intersted in searching.\n",
        "\n",
        "Let's find the percentage of male passengers that survived"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sP7JIT3xGv_s",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# get indices for male passengers\n",
        "field1 = 'Sex'\n",
        "field1_val = 'male'\n",
        "male_indices = np.where(data_numpy[0:,fields[field1]] == field1_val)\n",
        "male_indices = list(male_indices[0])\n",
        "\n",
        "# get indices for surviving passengers\n",
        "field2 = 'Survived'\n",
        "survived_indices = np.where(data_numpy[0:,fields[field2]] == '1')\n",
        "survived_indices = list(survived_indices[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndd2XElxGv_x"
      },
      "source": [
        "Now that we have a list of indices of passengers that survived, and a separate list of indices for the ones that are males, how can we use that information to find the number of male survivors?\n",
        "\n",
        "There are a couple ways we could do this. One option is to convert our lists of indices into sets and take advantage of the set intersection method/operator (i.e. &)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y2fZfDIGv_x"
      },
      "outputs": [],
      "source": [
        "# compute percentage that survived\n",
        "percent = 100*len(set(male_indices) & set(survived_indices))/len(male_indices)\n",
        "print(round(percent,2), '% of', field1_val, 'passengers survived')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcI4SfkzGv_z"
      },
      "source": [
        "We could do the same thing to find the percentage of female passengers that survived or even to find the percentage of first class, female passengeres who survived. But doing this would seem like a lot of code for each combination of characteristics. Why not write a function that generalizes?\n",
        "\n",
        "We are given some number characteristics (A, B, C) (e.g., A could be male, B could be first class, etc) and we want to find out the percentage of passengers with all of those characteristics that survived. The little algorithm we could write is:\n",
        "\n",
        "- find ind_A (the indices with characteristic A), ind_B, and ind_C and intersect them to form ind_characteristics\n",
        "- find ind_survived (the indices of all passengers who survived)\n",
        "- the length of ind_survived intersected with ind_characteristics divided by the length of ind_survived gives the proportion of surivors\n",
        "\n",
        "Since we want to be able to do this with any number of characteristics, lets put them in a list.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ggz5RhWLGv_0"
      },
      "outputs": [],
      "source": [
        "def get_survival(characteristics):\n",
        "    \"\"\"Return the percentage of passengers with the (field, value) entries in\n",
        "       characteristics that survived.\n",
        "       characteristics is a list of the form [(field, value), (field, value), ...]\n",
        "    \"\"\"\n",
        "    indices = set()\n",
        "    for i in range(len(characteristics)):\n",
        "        # get search category\n",
        "        field = characteristics[i][0]\n",
        "\n",
        "        # get value to search for\n",
        "        val = characteristics[i][1]\n",
        "\n",
        "        # find the matching indices\n",
        "        new_indices = set(list(np.where(data_numpy[0:,fields[field]] == val)[0]))\n",
        "\n",
        "        # intersect\n",
        "        if len(indices) == 0:\n",
        "            indices = new_indices\n",
        "        else:\n",
        "            indices &= new_indices\n",
        "\n",
        "    # find the indices of the survivors\n",
        "    indices_survived = set(list(np.where(data_numpy[0:,fields[\"Survived\"]] == \"1\")[0]))\n",
        "\n",
        "    return 100*len(indices_survived & indices)/len(indices)\n",
        "\n",
        "percent = get_survival([(\"Sex\", \"male\")])\n",
        "print(round(percent,2), '% of male passengers survived')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOsqLSQDGv_2"
      },
      "source": [
        "Now we can easily do the same thing for other combinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7lEfIAUGv_3"
      },
      "outputs": [],
      "source": [
        "# find the percentage of female passengers that survived\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx9YSSyeGv_6"
      },
      "source": [
        "How about we combine gender and class to see how many first class males survived compared to other classes. How could we do that?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVA-XNv-Gv_7"
      },
      "outputs": [],
      "source": [
        "# find the percentage of  male class 1 passengers that survived\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrVmuBRP6aMP"
      },
      "outputs": [],
      "source": [
        "# find the percentage of  female class 1 passengers that survived\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x-l_e8KGwAC"
      },
      "source": [
        "Let's now use the passenger gender and class to improve our prediction.\n",
        "\n",
        "#### Test Case 1:  Sample Male and Female Survivor\n",
        "  \n",
        "|Passenger|PassengerID|Pclass|Sex|Age|SibSp|Parch|Fare|Embarked_Val|\n",
        "|:--------|:----------|:-----|:--|:--|:----|:----|:---|:-----------|\n",
        "|Mrs. Laina Heikkinen|3|3|1|26|0|0|7.925|2|\n",
        "|Master. Michel Navratil|194|2|0|3|1|1|26|2|\n",
        "\n",
        "\n",
        "#### Test Case 2:  Sample Male and Female Non-Survivor\n",
        "\n",
        "|Passenger|PassengerID|Pclass|Sex|Age|SibSp|Parch|Fare|Embarked_Val|\n",
        "|:--------|:----------|:-----|:--|:--|:----|:----|:---|:-----------|\n",
        "|Miss. Augusta Planke|39|3|1|18|2|0|18|2|\n",
        "|Mr. Owen Harris Braund|1|3|0|22|1|0|7.25|2|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KRxL64FGwAD",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Estimate survival likelihood of the test case passengers and comment on the accuracy of the findings\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxR-Kf6DICa2"
      },
      "source": [
        "# Part 2\n",
        "\n",
        "# Problem: Visualizing Titanic Dataset\n",
        "\n",
        "This part will focus on visualizing the data to find patterns that may allow us to make a prediction on who would survive the Titanic tragedy.\n",
        "\n",
        "Python has many modules available dealing with visualization. One of the most popular to use is the **matplotlib module** which replicates the plotting capability of MATLAB (matplotlib is short for \"MATLAB plotting library). In what is to follow, we will discuss how to import and use this module.\n",
        "\n",
        "To start we can use the plot() method, which takes an optional format string argument that specifies the color and style of the plotted line. For example, plot(x_values, y_values, 'r--') uses 'r' to specify a red color, and '--' to specify a dashed line. You can find more information on formatting options at the following [link](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy8ezCAlICa2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40RNIZYAICa5"
      },
      "source": [
        "The program imports the pyplot module from the matplotlib package, renaming matplotlib.pyplot to plt using the **as** keyword.\n",
        "\n",
        "The plt.plot() function plots data onto the graph. plot() accepts various arguments. If provided just one list, as in plt.plot(val), plot() uses 0, 1, ... for x values, as in (0, val[0]), (1, val[1]), etc.\n",
        "\n",
        "plt.plot() on its own will not display anyting. One needs to call the plt.show() function to displays the graph.\n",
        "\n",
        "To start let's plot the survival percentages based on gender:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YH4XytTICa6"
      },
      "outputs": [],
      "source": [
        "# plot survival by gender\n",
        "male_survived = 18.9\n",
        "female_survived = 74.2\n",
        "survived_percent = [male_survived, female_survived]\n",
        "\n",
        "# plot survival percentages\n",
        "plt.plot(survived_percent)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5qKZDKgICa8"
      },
      "outputs": [],
      "source": [
        "# plot survival with dotted connecting lines\n",
        "plt.plot(survived_percent, 'm--o')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGRaAb42ICa_"
      },
      "outputs": [],
      "source": [
        "# plot survival without connecting lines\n",
        "plt.plot(survived_percent, 'mo', markerfacecolor = 'None')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ni03fpXRICbC"
      },
      "source": [
        "Calling plot multiple times draws multiple lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ_LRRmoICbD"
      },
      "outputs": [],
      "source": [
        "# we can plot percentages of those that survived with overlapping percentages\n",
        "# of those that did not survive\n",
        "survived_percent = [male_survived, female_survived]\n",
        "n_survived_percent = [100 - male_survived, 100 - female_survived]\n",
        "\n",
        "plt.plot(survived_percent, 'm--o')\n",
        "plt.plot(n_survived_percent, 'r-.D')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WKP8oSSICbF"
      },
      "source": [
        "#### Text and Annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IAE_a1sICbG"
      },
      "outputs": [],
      "source": [
        "# add title and axis labels\n",
        "plt.plot(survived_percent, 'm--o')\n",
        "plt.title('Survival Percentage Based on Gender')\n",
        "plt.ylabel('Percentage')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIEXlHj7ICbJ"
      },
      "outputs": [],
      "source": [
        "# add tick labels\n",
        "plt.plot(survived_percent, 'm--o')\n",
        "plt.xticks([0, 1],['males', 'females'])\n",
        "plt.title('Survival Percentage Based on Gender')\n",
        "plt.xlabel('Gender')\n",
        "plt.ylabel('Percentage')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Db7OWO4QICbM"
      },
      "source": [
        "Hmmmm, this information would be best represented as a bar graph. Turns out we can do that as well using matlibplot.\n",
        "\n",
        "#### Bar Graphs - Averaged Data\n",
        "We can visualize the survival rates using bar graphs as shown:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVb5LeZ4ICbN"
      },
      "outputs": [],
      "source": [
        "# plot bar graph of survival by gender\n",
        "pos = [0, 1]\n",
        "plt.bar(pos, survived_percent, align = 'center')\n",
        "plt.xticks(pos,['males survived', 'females survived'])\n",
        "plt.title('Survival Percentage Based on Gender')\n",
        "plt.xlabel('Gender')\n",
        "plt.ylabel('Percentage')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNd5Tbe6ICbQ"
      },
      "outputs": [],
      "source": [
        "# plot bar graph of survival by class\n",
        "male_class_survived = [36.88, 15.74, 13.54]\n",
        "\n",
        "\n",
        "# x axis position of bars graph\n",
        "pos = range(len(male_class_survived))\n",
        "# generate bar graph\n",
        "plt.bar(pos, male_class_survived, align = 'center')\n",
        "# provide labels for each bar based on provided positions\n",
        "plt.xticks(pos,['1st', '2nd', '3rd'])\n",
        "plt.title('Survival Percentage of Males Based on Class')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Percentage')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkLuyfMmICbT"
      },
      "source": [
        "**(optional) We can also use subplots to plot everything together. In your spare time see if you can plot the percentage of male and female survivors using subplots.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Md2BUXqICbU"
      },
      "outputs": [],
      "source": [
        "# (optional) use subplots to show male and female survivors by class\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCvP1KOHICbW"
      },
      "source": [
        "What if we wanted to plot an entire column? For example, we could print the ages of all the passengers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA03UisEICbY"
      },
      "source": [
        "#### Numpy: Converting Column of str to int\n",
        "When we loaded our data all the values were converted into strings because unlike lists numpy arrays can be of only one type (i.e. string or float, not both). This is somewhat problematic as we cannot plot strings, we need numerical values. We need to fix our dataset before we can plot it. How might we do that?\n",
        "\n",
        "If we want to plot the age of passengers we need to apply the following steps:\n",
        "\n",
        "1. Select the age column\n",
        "2. find all the missing ages and replace them with a numerical value (i.e. np.nan)\n",
        "3. convert the numpy array into a float\n",
        "\n",
        "Selecting the age column is something we've done, but what about finding and overwriting data? One option could be to find the indicies that are blank and overwrite them. There is an easier way to do this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAiGLmgKICbZ"
      },
      "source": [
        "Turns out we can also select numpy data by value using conditionals. For examples:\n",
        "    \n",
        "    x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "\n",
        "    x[x >= 5]\n",
        "    \n",
        "Will select only the data that is greater than or equal to 5 in a numpy array x.\n",
        "\n",
        "We can take this further by assigning that data a particular value.\n",
        "\n",
        "    x[x >= 5] = 100\n",
        "\n",
        "Will select only the data greater than or equal to 5 and change the value to 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4htYcx6KICbe"
      },
      "outputs": [],
      "source": [
        "x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "print(x[x >= 5])\n",
        "x[x >= 5] = 100\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gffNtq2HICbg"
      },
      "source": [
        "Now we can use the same technique to update our age data to numerical values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBLhsTkAICbh",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# select the age of passengers\n",
        "index = fields['Age']\n",
        "age = data_numpy[1:, index]\n",
        "\n",
        "# find the ones that are empty and make them nan\n",
        "age[age == ''] = np.nan\n",
        "\n",
        "# convert all the strings into floats\n",
        "age = age.astype(float)\n",
        "\n",
        "# verify conversion to float\n",
        "print(age)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEYXB7RAICbj"
      },
      "source": [
        "By making the missing data of type nan (not a number), plot will ignore those values when plotting age values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40Odp_51ICbk"
      },
      "outputs": [],
      "source": [
        "# plot age\n",
        "plt.plot(age)\n",
        "plt.title('Passenger Age')\n",
        "plt.xlabel('Passenger #')\n",
        "plt.ylabel('Age (years)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqgU-KfyICbm"
      },
      "source": [
        "Since we don't care about the sequence of the age of passengers, it may be more informative to see how many passengers we have within each age group, i.e. plot a histogram of our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtbXEFPLICbn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.hist(age, bins=30)\n",
        "plt.title('Passenger Age Histogram')\n",
        "plt.xlabel('Age (years)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAz5VuqbICbp"
      },
      "source": [
        "Hmmmm, seems like we obtain a histogram for nan values. How can we fix this? Let's search on Google to see if anyone else has had this problem. A quick search reveals the following stackoverflow discussion [(link)](http://stackoverflow.com/questions/11620914/removing-nan-values-from-an-array).\n",
        "\n",
        "    x = x[numpy.logical_not(numpy.isnan(x))]\n",
        "\n",
        "or   \n",
        "\n",
        "    x = x[~numpy.isnan(x)]\n",
        "\n",
        "This is actually the same as:\n",
        "\n",
        "    x = x[numpy.isnan(x) == 0]\n",
        "\n",
        "Let's use this to select all age values excluding the nan type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY6rn43FICbq"
      },
      "outputs": [],
      "source": [
        "# plot a histogram of passenger ages excluding nan values\n",
        "# to plot the histogram we need to exclude the missing ages, i.e. nan values\n",
        "# the np.isnan() method returns True for values that are of type nan\n",
        "plt.hist(age[np.isnan(age) == 0], bins=30)\n",
        "plt.title('Passenger Age Histogram')\n",
        "plt.xlabel('Age (years)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOaQ4wN8ICby"
      },
      "source": [
        "**(Optional) As another exploration activity, it might be useful to see how survival changes with age. To do that it might be helpful to plot the survivor and non-survivor age histograms overtop of each other. Hmmm, how might we do that?** [(Hint)](http://stackoverflow.com/questions/6871201/plot-two-histograms-at-the-same-time-with-matplotlib)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uybQTudDICbz"
      },
      "outputs": [],
      "source": [
        "# (Optional) plot a histogram of passenger ages excluding nan values and overlap survivors and non-survivors. Does this provide any insight into the data?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLjhsu20ICb3"
      },
      "source": [
        "Including age with the class and gender may benefit our predictions on the test cases. It would take some work to implement this change. It may be a good idea to pause and reconsider our approach continuing any further.\n",
        "\n",
        "In the next part we will discuss how to take the information we have gained about our dataset to apply commonly used machine learning algorithms to predict passenger survival outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdF9MqWLIhb8"
      },
      "source": [
        "# Part 3\n",
        "\n",
        "# Problem: Predicting Titanic Survivors\n",
        "\n",
        "After visualizing the data in the previous part, we can see that the odds of surviving vary depending on passenger information such as: age, sex, class. Is there some way we can use all our passenger information to predict survival?\n",
        "\n",
        "In this part we will take a look at how to use the readily available machine learning algorithms to make predictions on survival.\n",
        "\n",
        "The machine learning community has grown substantially over the years and there are many modules available to implement the different algorithms. To implement the algorithms, all we need to do is obtain a dataset and arrange it to follow the machine learning conventions which can be described by the following algorithm plan:\n",
        "\n",
        "### End-to-End Machine Learning Project:\n",
        "\n",
        "* Look at the big picture\n",
        "+ Get the data\n",
        "+ Visualize the data to gain insights\n",
        "+ Preprocess (clean) data to ensure all values are numerical\n",
        "+ Divide dataset into a training and testing set\n",
        "+ Select machine learning model and train it\n",
        "+ Perform prediction on the testing data\n",
        "+ Evaluate prediction performance of machine learning algorithm (use new data or holdout set to be sure of performance)\n",
        "\n",
        "Note that we've already completed steps 1 - 3 and we've already done a bit of work on step 4 when we ploted age of passengers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCN-VX6BE3yD"
      },
      "source": [
        "#### Replace strings and missing values with numbers\n",
        "\n",
        "As part of the data structure we require that all the samples are numerical. We need to prepare data by removing strings and filling in missing values.\n",
        "\n",
        "As we saw previously, we can select numpy data by value using conditionals. For examples:\n",
        "\n",
        "    x[numpy.isnan(x) == 0]\n",
        "\n",
        "Will select only the data that is not of type nan. Similarly,\n",
        "\n",
        "    x[x >= 5]\n",
        "    \n",
        "Will select only the data that is greater than or equal to 5 in a numpy array x.\n",
        "\n",
        "We can take this further by assigning that data a particular value. For example:\n",
        "\n",
        "    x[x >= 5] = 100\n",
        "\n",
        "Will select only the data greater than or equal to 5 and change the value to 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7ivBqhOIhcE",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# any value >= 5 will be replaced with 100\n",
        "x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
        "x[x >= 5] = 100\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5lzY8hOIhcI"
      },
      "source": [
        "Now let's use that to fix our data to only hold numerical values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STrkVhxlIhcR"
      },
      "source": [
        "Let's put everything together into a class data structre and test. **Make sure that the trian.csv file is loaded.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAAB_wwiIhcS",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Helper Titanic_Data Class\n",
        "\n",
        "# Correct values in Survival, Gender, Embarked, and Age columns\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "class Titanic_Data:\n",
        "    \"\"\"Titanic data set\"\"\"\n",
        "\n",
        "    def __init__(self, Filename):\n",
        "        \"\"\"load Titanic data set\"\"\"\n",
        "        with open(Filename,'r') as csvfile:\n",
        "            data_reader = csv.reader(csvfile)\n",
        "            data_orig = []\n",
        "            for row in data_reader:\n",
        "                data_orig.append(row)\n",
        "\n",
        "        # loop through field names and populate a dictionary with indices\n",
        "        fields = {}\n",
        "        for i in range(len(data_orig[0])):\n",
        "            fields[data_orig[0][i]] = i\n",
        "\n",
        "        # exclude the first row when preparing the numpy data structure\n",
        "        self.data = np.array(data_orig[1:])\n",
        "        self.fields = fields\n",
        "\n",
        "    def get_survival(self, characteristics):\n",
        "        \"\"\"Return the percentage of passengers with the (field, value) entries in\n",
        "           characteristics that survived.\n",
        "           characteristics is a list of the form [(field, value), (field, value), ...]\n",
        "        \"\"\"\n",
        "        indices = set()\n",
        "        for i in range(len(characteristics)):\n",
        "            # obtain search category\n",
        "            field = characteristics[i][0]\n",
        "\n",
        "            # obtain value to search for\n",
        "            val = characteristics[i][1]\n",
        "\n",
        "            # find and intersect the matching indices\n",
        "            new_indices = set(list(np.where(self.data[0:,self.fields[field]] == val)[0]))\n",
        "\n",
        "            # intersect\n",
        "            if len(indices) == 0:\n",
        "                indices = new_indices\n",
        "            else:\n",
        "                indices &= new_indices\n",
        "\n",
        "        # find the indices of the survivors\n",
        "        indices_survived = set(list(np.where(self.data[0:,self.fields[\"Survived\"]] == \"1\")[0]))\n",
        "\n",
        "        return 100*len(indices_survived & indices)/len(indices)\n",
        "\n",
        "    def clean_data(self):\n",
        "        \"\"\"Converts all data into numerical values\n",
        "        (missing data is converted into nan)\"\"\"\n",
        "        self.clean('Sex', ['male', 'female'])\n",
        "        self.clean('Embarked', ['C', 'Q', 'S'])\n",
        "        self.clean('Age')\n",
        "        self.clean('Pclass')\n",
        "        self.clean('SibSp')\n",
        "        self.clean('Parch')\n",
        "        self.clean('Fare')\n",
        "\n",
        "    def clean(self, col_header, values = []):\n",
        "        \"\"\"Converts column data into numerical values\n",
        "        (missing data is convereted into nan)\"\"\"\n",
        "        # select the column\n",
        "        column = self.data[:,self.fields[col_header]]\n",
        "        # find the ones that are empty and make them nan\n",
        "        column[column == ''] = np.nan\n",
        "        # encode the the strings as numbers\n",
        "        for i in range(len(values)):\n",
        "            column[column == values[i]] = i\n",
        "        # overwrite\n",
        "        self.data[:,self.fields[col_header]] = column\n",
        "\n",
        "    def keep_columns(self, L):\n",
        "        \"\"\"Select Features \"\"\"\n",
        "        feature_data = self.data[:,L]\n",
        "        feature_data = feature_data.astype(float)\n",
        "        return feature_data\n",
        "\n",
        "# call function to prepare data structure\n",
        "titanic = Titanic_Data('train.csv')\n",
        "print(titanic.data[0,:])\n",
        "\n",
        "# cleaned data\n",
        "titanic.clean_data()\n",
        "print(titanic.data[0,:])\n",
        "\n",
        "# remove unnecessary columns and convert array to float\n",
        "feature_data = titanic.keep_columns([1, 2, 4, 5, 6, 7, 9, 11])\n",
        "print(feature_data[0,:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il8Y4yURIhcV"
      },
      "source": [
        "#### Divide data into a training and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J7GmA54IhcV"
      },
      "outputs": [],
      "source": [
        "feature_data = titanic.keep_columns([1, 2, 4, 5, 6, 7, 9, 11])\n",
        "\n",
        "# replace all nan values with -1 to prevent an error\n",
        "feature_data[np.isnan(feature_data)==1] = -1\n",
        "\n",
        "# segment data into training and testing datasets (features)\n",
        "halfway = len(feature_data)//2\n",
        "training_data = feature_data[0:halfway,1:]\n",
        "testing_data = feature_data[halfway:,1:]\n",
        "\n",
        "# segment data into training and testing labels (targets)\n",
        "training_labels = feature_data[0:halfway,0]\n",
        "testing_labels = feature_data[halfway:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFY-O9uZIhcY"
      },
      "outputs": [],
      "source": [
        "# verify the training data and labels are correct\n",
        "print(training_data)\n",
        "print(training_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPGDorMdIhcb"
      },
      "source": [
        "#### Import a Machine Learning Algorithm\n",
        "There are many machine learning algorithms developed for making predictions (classification) similar to the one we are trying to do in this design problem. To use these algorithms we first need to import them from the scikit-learn machine learning modules for Python. More information on the different algorithms can be found at the following [link](http://scikit-learn.org/stable/).\n",
        "\n",
        "There are many machine algorithms to choose from such as:\n",
        "\n",
        "1. Decision Trees\n",
        "2. Random Forests\n",
        "3. Support Vector Machines\n",
        "4. NaÃ¯ve Bayes\n",
        "\n",
        "For now let us focus on the decision trees classifier. We import the classifier and setup some default parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4DoG2TrIhcc"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier(max_depth=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVayKTwiIhcf"
      },
      "source": [
        "#### Train machine learning algorithm\n",
        "Next we train our algorithm on the training set. This updates the model parameters to make predictions specific to our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R0A5IsGIhcg"
      },
      "outputs": [],
      "source": [
        "model.fit(training_data, training_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkRki_HsIhci"
      },
      "source": [
        "#### Perform prediction on the test cases.\n",
        "Now that model is trained, we can use it to predict the outcome on our test cases:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8ab_Sl0Ihcj"
      },
      "source": [
        "#### Test Case 1:  Sample Male and Female Survivor\n",
        "  \n",
        "|Passenger|PassengerID|Pclass|Sex|Age|SibSp|Parch|Fare|Embarked_Val|\n",
        "|:--------|:----------|:-----|:--|:--|:----|:----|:---|:-----------|\n",
        "|Mrs. Laina Heikkinen|3|3|1|26|0|0|7.925|2|\n",
        "|Master. Michel Navratil|194|2|0|3|1|1|26|2|\n",
        "\n",
        "#### Test Case 2:  Sample Male and Female Non-Survivor\n",
        "\n",
        "|Passenger|PassengerID|Pclass|Sex|Age|SibSp|Parch|Fare|Embarked_Val|\n",
        "|:--------|:----------|:-----|:--|:--|:----|:----|:---|:-----------|\n",
        "|Miss. Augusta Planke|39|3|1|18|2|0|18|2|\n",
        "|Mr. Owen Harris Braund|1|3|0|22|1|0|7.25|2|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XmBTJZoIhck"
      },
      "source": [
        "We first need to select our testcase samples. We can do that by obtaining the samples from the original feature data by providing their order in the list (i.e. PassengerID)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DK6GY878Ihcl",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# select test case 1 and 2 passengers\n",
        "passenger_ids = np.array([3, 194, 39, 1])\n",
        "print(passenger_ids-1)\n",
        "testcase_data = feature_data[passenger_ids-1, 1:]\n",
        "\n",
        "# display details on selected passengers\n",
        "print(testcase_data)\n",
        "\n",
        "# predict survival outcome\n",
        "testcase_predict = model.predict(testcase_data)\n",
        "print('Predicted Survival Outcome: ', testcase_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqGVEsRnIhcn"
      },
      "source": [
        "On the above sample we see that our algorithm was able to successfully predict the survival outcome of the passengers in the test cases.\n",
        "\n",
        "Wait a second... it seems like all our test case samples are also in the training data. The algorithm was trained on the same data, which means they're going to do better on this data than on completely new data (i.e. testing_data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3die4ahIhco"
      },
      "source": [
        "Let's compare the prediction on the training_data and testing_data to get a proper validation of prediction performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDDZHy5IIhcp",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# predict the first 10 training samples and compare to the actual survival outcomes\n",
        "print('Training Sample Labels:     ', training_labels[0:10])\n",
        "sample_predict = model.predict(training_data[0:10,:])\n",
        "print('Predicted Survival Outcome: ', sample_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IhI8XrqIhcr"
      },
      "source": [
        "Seems like the prediction worked well on the training samples. How about the testing samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bL1GOG-Ihcs",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# test on the first 10 testing samples and compare to the actual survival outcomes\n",
        "print('Testing Sample Labels:      ', testing_labels[0:10])\n",
        "sample_predict = model.predict(testing_data[0:10,:])\n",
        "print('Predicted Survival Outcome: ', sample_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiMLqlBeIhcu"
      },
      "source": [
        "Already we see that some of the predictions were not correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw2mTBphIhcv"
      },
      "source": [
        "#### Evaluate performance by computing the percentage of correctly predicted survival outcomes.\n",
        "To get a true evaluation of performance we need to test on a larger set of data. Let's predict the outcome on all of the testing data and compute a percentage of how many survival outcomes were correctly predicted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7MWiQgpIhcw",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# obtain survival predictions on all testing data\n",
        "testing_predicted = model.predict(testing_data)\n",
        "\n",
        "# obtain a percentage score of performance on all testing data\n",
        "score = 100*(1-sum(abs(testing_predicted-testing_labels))/len(testing_predicted))\n",
        "print('Testing data performance', score, '% correctly predicted')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78GQQvf7Ihcz"
      },
      "source": [
        "How does that compare with the samples obtained from the training data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVci4fKtIhc1"
      },
      "outputs": [],
      "source": [
        "# obtain survival predictions on all training data\n",
        "training_predicted = model.predict(training_data)\n",
        "\n",
        "# obtain a percentage score of performance on all training data\n",
        "score = 100*(1-sum(abs(training_predicted-training_labels))/len(training_predicted))\n",
        "print('Training data performance', score, '% correctly predicted')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRkTP0i4Ihc6"
      },
      "source": [
        "The prediction achieved better performance, 85.4% on the training data than the testing data 77.6%. This makes sense because the machine learning algorithms are trying to model the training data not the testing data.  \n",
        "\n",
        "Let's see if we can get better performance by adjusting the training parameters (i.e. max_depth) and applying other machine learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWfdJO_uIhc7"
      },
      "source": [
        "### Compare Prediction Performance Across Machine Learning Algorithms\n",
        "\n",
        "As a final step we will adjust our training parameters and machine learning algorithms to see if we can do any better on the survival prediction performance.\n",
        "\n",
        "Test **Decision Tree** Learning Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giGTsozmIhc8"
      },
      "outputs": [],
      "source": [
        "# Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier(max_depth=15)\n",
        "\n",
        "# Fit the model to our training data\n",
        "model.fit(training_data, training_labels)\n",
        "\n",
        "# Make predictions\n",
        "testing_predicted = model.predict(testing_data)\n",
        "score = 100*(1-sum(abs(testing_predicted-testing_labels))/len(testing_predicted))\n",
        "print(\"DT Test:\", score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GyLcmOoIhdA"
      },
      "source": [
        "There you go, youâ€™ve just implemented a high level prediction in only a couple lines. Since each dataset is different, we may find that there are other algorithms that are better suited for this prediction. Letâ€™s examine some of the other popular machine learning algorithms.\n",
        "\n",
        "Test **Random Forest** Learning Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9nlnxGsIhdB"
      },
      "outputs": [],
      "source": [
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=250)\n",
        "\n",
        "# Fit the model to our training data\n",
        "model.fit(training_data, training_labels)\n",
        "\n",
        "# Make predictions\n",
        "testing_predicted = model.predict(testing_data)\n",
        "score = 100*(1-sum(abs(testing_predicted-testing_labels))/len(testing_predicted))\n",
        "print(\"RF Test:\", score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc0w9z5ZIhdD"
      },
      "source": [
        "Test **Support Vector Machine** Learning Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sck7X0q7IhdE"
      },
      "outputs": [],
      "source": [
        "# Support Vector Machines\n",
        "from sklearn import svm\n",
        "model = svm.SVC(gamma=2, C=1)\n",
        "\n",
        "# Fit the model to our training data\n",
        "model.fit(training_data, training_labels)\n",
        "\n",
        "# Make predictions\n",
        "testing_predicted = model.predict(testing_data)\n",
        "score = 100*(1-sum(abs(testing_predicted-testing_labels))/len(testing_predicted))\n",
        "print(\"SVM Test:\", score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-IHgAHlIhdI"
      },
      "source": [
        "Test **NaÃ¯ve Bayes** Machine Learning Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AB2P3_mJIhdI",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "model = GaussianNB()\n",
        "\n",
        "# Fit the model to our training data\n",
        "model.fit(training_data, training_labels)\n",
        "\n",
        "# Make predictions\n",
        "testing_predicted = model.predict(testing_data)\n",
        "score = 100*(1-sum(abs(testing_predicted-testing_labels))/len(testing_predicted))\n",
        "print(\"NB Test:\", score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCEI3EXaIhdK"
      },
      "source": [
        "These are just a few of the available machine learning algorithms at our disposal. Designing these algorithms would have taken hours of work, fortunately, the Python open source community is strong and many people out there are willing to contribute. Hence, all we have to do is change one or two lines in our code to evaluate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPmKK-WoIhdL"
      },
      "source": [
        "## Perform Final Testing\n",
        "\n",
        "From the above algorithms tested, it seems like the Naive Bayes performed the best.  \n",
        "\n",
        "Now that we have selected our machine learning model for predicting survival, we can finally answer the question of whether or not Jack and Rose would have survived (test case 3). To make this prediction we need to provide information on the passengers in the expected form:\n",
        "\n",
        "|Passenger|Pclass|Sex|Age|SibSp|Parch|Fare|Embarked|\n",
        "|:--------|:-----|:--|:--|:----|:----|:---|:-----------|\n",
        "|Jack|3|0|25|0|0|7|2|\n",
        "|Rose|1|1|22|1|0|50|2|\n",
        "\n",
        "Given the provided information about Jack and Rose, would they have survived the tragedy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8Zf8QIJIhdM"
      },
      "outputs": [],
      "source": [
        "# Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier(max_depth=15)\n",
        "\n",
        "# Fit the model to our training data\n",
        "model.fit(training_data, training_labels)\n",
        "\n",
        "# Create test data (features of Jack and Rose)\n",
        "test_jack = np.array([3, 0, 25, 0, 0, 7, 2])\n",
        "test_rose = np.array([1, 1, 22, 1, 0, 50, 2])\n",
        "\n",
        "testing_predicted = model.predict(test_jack.reshape(1,-1))\n",
        "print(\"Jack Survival:\", testing_predicted)\n",
        "\n",
        "testing_predicted = model.predict(test_rose.reshape(1,-1))\n",
        "print(\"Rose Survival:\", testing_predicted)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R09EqHKrIhdO"
      },
      "source": [
        "Our model **prediction is that Rose would survive and Jack would not**. You should watch the movie to find out if the prediction is correct.\n",
        "\n",
        "### Discussion\n",
        "Given that the accuracy for all of our models is maxing out around 80%, it will be interesting to look at specific passengers for whom these prediction algorithms are incorrect. Provided is a list of some of the passengers that were incorrectly predicted:\n",
        "\n",
        "#### Allison family\n",
        "For instance, three incorrectly classified passengers are members of the Allison family, who perished even though the model predicted that they would survive. These first class passengers were very wealthy, as can be evidenced by their far-above-average ticket prices. For Betsy (25) and Loraine (2) in particular, not surviving is very surprising, considering that we found earlier that over 96% of first class women lived through the disaster.  \n",
        "\n",
        "So what happened? A surprising amount of information on each Titanic passenger is available online; it turns out that the Allison family was unable to find their youngest son Trevor and was unwilling to evacuate the ship without him. Tragically, Trevor was already safe in a lifeboat with his nurse and was the only member of the Allison family to survive the sinking.\n",
        "\n",
        "#### John Jacob Astor\n",
        "Another interesting example is John Jacob Astor, who perished in the disaster even though the model predicted he would survive. Astor was the wealthiest person on the Titanic, an impressive feat on a ship full of multimillionaire industrialists, railroad tycoons, and aristocrats. Given his immense wealth and influence, which the model may have deduced from his ticket fare (valued at over \\$35,000 in 2016 dollars), it seems likely that he would have been among the 35 percent of men in first class to survive. However, this was not the case: although his pregnant wife survived, John Jacob Astorâ€™s body was recovered a week later, along with a gold watch, a diamond ring with three stones, and no less than \\$92,481 (2016 value) in cash.\n",
        "\n",
        "On the other end of the spectrum is Olaus Jorgensen Abelseth, a 25-year-old Norwegian sailor. Abelseth, as a man in 3rd class, was not expected to survive by our classifier. Once the ship sank, however, he was able to stay alive by swimming for 20 minutes in the frigid North Atlantic water before joining other survivors on a waterlogged collapsible boat and rowing through the night. Abelseth got married three years later, settled down as a farmer in North Dakota, had 4 kids, and died in 1980 at the age of 94.\n",
        "\n",
        "\n",
        "### Conclusions\n",
        "Initially I was disappointed by the accuracy of our machine learning models maxing out at about 80% for this data set. Itâ€™s easy to forget that these data points each represent real people, each of whom found themselves stuck on a sinking ship without enough lifeboats. When we looked into data points for which our model was wrong, we can uncover incredible stories of human nature driving people to defy their logical fate. It is important to never lose sight of the human element when analyzing this type of data set. This principle will be especially important going forward, as machine learning is increasingly applied to human data sets by organizations such as insurance companies, big banks, and law enforcement agencies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4Vceh0OwtJX"
      },
      "source": [
        "## Take home questions\n",
        "\n",
        "(1) In performing our testing we have made some fundamental mistakes that results in overfitting to our 4 test cases. Spend some time looking through how we setup our training and testing data, in particular take a look at the test cases. Indicate what the problem is and suggest some ways to correct it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TfXAbGawsHO"
      },
      "outputs": [],
      "source": [
        "# correct the above test process and evaluate the performance of the algorithms\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVBrnTnexzJM"
      },
      "source": [
        "(2) Exploration! Compare different machine learning algorithms and data preprocessing to see if you can achieve a better prediction accuracy. How can you be sure that you are not overfitting to the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SEc_xNwyITV"
      },
      "outputs": [],
      "source": [
        "# write your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}