{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G53tY4PwB1ZD"
      },
      "source": [
        "## Tutorial 4 - Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De25YpsjO8bq"
      },
      "source": [
        "## Autoencoder\n",
        "An autoencoder is not used for *supervised learning*. We will\n",
        "no longer try to *predict* something about our input.\n",
        "Instead, an autoencoder is considered a **generative model**:\n",
        "it learns a distributed *representation* of our training data,\n",
        "and can even be used to generate new instances of the training data.\n",
        "\n",
        "An autoencoder model contains two components:\n",
        "\n",
        "- An **encoder** that takes an image as input, and\n",
        "  outputs a low-dimensional embedding (representation)\n",
        "  of the image.\n",
        "- A **decoder** that takes the low-dimensional embedding,\n",
        "  and reconstructs the image.\n",
        "\n",
        "An autoencoder is typically shown like below:\n",
        "\n",
        "![alt text](https://i2.wp.com/www.statworx.com/wp-content/uploads/mushroom_encoder.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvwWsXRrV9IK"
      },
      "source": [
        "Autoencoders contain an encode stage which is similar to what we have seen with our ANNs and CNNs, followed by a decode stage which is just the reverse of the encode stage. In presenting the architecture of autoencoders we will try to use code that we have seen when working with MNIST data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv_yiMnlPCVL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "mnist_data = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_data = list(mnist_data)[:4096]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ccpZjeFWpI2"
      },
      "source": [
        "#### Architecture\n",
        "The architecture is very similar to what we have seen in the past, except now the output will be the same size as the input. Notice also that we apply a sigmoid on the output data, this is to scale the output from 0 to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ayj6rmY1PPP2"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        encoding_dim = 32\n",
        "        # encoder\n",
        "        self.fc1 = nn.Linear(28 * 28, encoding_dim)\n",
        "        # decoder\n",
        "        self.fc2 = nn.Linear(encoding_dim, 28*28)\n",
        "\n",
        "    def forward(self, img):\n",
        "        flattened = img.view(-1, 28 * 28)\n",
        "        x = F.relu(self.fc1(flattened))\n",
        "        # sigmoid for scaling output from 0 to 1\n",
        "        x = F.sigmoid(self.fc2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gJr7RRyW6r7"
      },
      "source": [
        "#### Training an Autoencoder\n",
        "\n",
        "How do we train an autoencoder? How do we know what\n",
        "kind of \"encoder\" and \"decoder\" we want?\n",
        "\n",
        "One observation is that if we pass an image through the encoder,\n",
        "then pass the result through the decoder, we should get\n",
        "roughly the same image back. Ideally, reducing the\n",
        "dimensionality and then generating the image should\n",
        "give us the same result.\n",
        "\n",
        "This observation provides us a training strategy: we will\n",
        "minimize the reconstruction error of the autoencoder\n",
        "across our training data.\n",
        "We use a loss function called 'MSELoss', which\n",
        "computes the square error at every pixel.\n",
        "\n",
        "Beyond using a different loss function, the training\n",
        "scheme is roughly the same. Note that in the code below,\n",
        "we are using a the optimizer called 'Adam'.\n",
        "\n",
        "We switched to this optimizer not because it is specifically\n",
        "used for autoencoders, but because this is the optimizer that\n",
        "people tend to use in practice. Feel free to use Adam for your\n",
        "other neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G16lOUg4PSsc"
      },
      "outputs": [],
      "source": [
        "def train(model, num_epochs=5, batch_size=64, learning_rate=1e-3):\n",
        "    torch.manual_seed(42)\n",
        "    criterion = nn.MSELoss() # mean square error loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=learning_rate,\n",
        "                                 weight_decay=1e-5) # <--\n",
        "    train_loader = torch.utils.data.DataLoader(mnist_data,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True)\n",
        "    outputs = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for data in train_loader:\n",
        "            img, _ = data\n",
        "            recon = model(img)\n",
        "            img = img.view(-1, 28 * 28)\n",
        "            loss = criterion(recon, img)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n",
        "        outputs.append((epoch, img, recon),)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2V_of0xPVja"
      },
      "outputs": [],
      "source": [
        "model = Autoencoder()\n",
        "max_epochs = 20\n",
        "outputs = train(model, num_epochs=max_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqwvddXBPaeW"
      },
      "source": [
        "Just like with our ANN we can have additional layers to make a deep autoencoder, also known as a stacked autoencoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Raad-Oe_P0Qn"
      },
      "source": [
        "## Convolutional Autoencoder\n",
        "When working with image data it is often better to use a convolutional neural network and take advantage of the spatial relationships. The architecture for the encoder stage of a convolutional autoencoder will consist of standard convolutional layers that we have seen in our previous architectures. The decoder step will be a bit more tricky since we need a way to increase the resolution.  \n",
        "\n",
        "We need something akin to convolution, but that goes in the *opposite* direction. We will use something called a **transpose convolution**. Transpose convolutions were first called *deconvolutions*, since it is the ``inverse'' of a convolution operation. However, the terminology was confusing since it has nothing to do with the mathematical notion of deconvolution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4fH9NRNRvEY"
      },
      "source": [
        "### Convolution Transpose\n",
        "\n",
        "First, let's illustrate how convolution transposes can be \"inverses\" of convolution layers.\n",
        "We begin by creating a convolutional layer in PyTorch. This is the convolution that we will\n",
        "try to find an \"inverse\" for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsT5hl7URvEZ"
      },
      "outputs": [],
      "source": [
        "conv = nn.Conv2d(in_channels=8,\n",
        "                 out_channels=8,\n",
        "                 kernel_size=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iguPJQK5RvEd"
      },
      "source": [
        "To illustrate how convolutional layers work, we'll create a random tensor\n",
        "and see how the convolution acts on that tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5CbsKUpRvEe"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(2, 8, 64, 64)\n",
        "y = conv(x)\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UHORk9jRvEh"
      },
      "source": [
        "A convolution transpose layer with the exact same specifications as above\n",
        "would have the \"reverse\" effect on the shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us04SDyfRvEi"
      },
      "outputs": [],
      "source": [
        "convt = nn.ConvTranspose2d(in_channels=8,\n",
        "                           out_channels=8,\n",
        "                           kernel_size=5)\n",
        "convt(y).shape # should be same as x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEUnPda_RvEk"
      },
      "source": [
        "And it does! Notice that the weights of this convolution transpose layer are all\n",
        "random, and are unrelated to the weights of the original `Conv2d`. So, the layer\n",
        "`convt`  is not the mathematical inverse of the layer `conv`. However, with training,\n",
        "the convolution transpose has the potential to learn to act as an approximate\n",
        "inverse to `conv`.\n",
        "\n",
        "Here is another example of `convt` in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjmTfvRlRvEm"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(32, 8, 64, 64)\n",
        "y = convt(x)\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVaRmsaTRvEp"
      },
      "source": [
        "Notice that the width and height of `y` is `68x68`, because the `kernel_size` is 5\n",
        "and we have not added any padding. You can verify that if we start with a tensor\n",
        "with resolution `68x68` and applied a `5x5` convolution, we would end up with\n",
        "a tensor with resolution `64x64`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3O-g8rVRvEq"
      },
      "outputs": [],
      "source": [
        "conv = nn.Conv2d(in_channels=8,\n",
        "                 out_channels=16,\n",
        "                 kernel_size=5)\n",
        "y = torch.randn(32, 8, 68, 68)\n",
        "x = conv(y)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgIkUJMFRvEt"
      },
      "source": [
        "As before, we can add a padding to our convolution transpose, just like we added\n",
        "padding to our convolution operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJCX_kfURvEu"
      },
      "outputs": [],
      "source": [
        "convt = nn.ConvTranspose2d(in_channels=16,\n",
        "                           out_channels=8,\n",
        "                           kernel_size=5,\n",
        "                           padding=2)\n",
        "x = torch.randn(32, 16, 64, 64)\n",
        "y = convt(x)\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdGWsOYwRvEx"
      },
      "source": [
        "More interestingly, we can add a stride to the convolution to increase our resolution!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct3nMkLBRvEy"
      },
      "outputs": [],
      "source": [
        "convt = nn.ConvTranspose2d(in_channels=16,\n",
        "                           out_channels=8,\n",
        "                           kernel_size=5,\n",
        "                           stride=2,\n",
        "                           output_padding=1, # needed because stride=2\n",
        "                           padding=0)\n",
        "x = torch.randn(32, 16, 64, 64)\n",
        "y = convt(x)\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhhTNj1uRvE0"
      },
      "source": [
        "Our resolution has doubled.\n",
        "\n",
        "But what is actually happening? Essentially, we are adding a padding of zeros\n",
        "in between every row and every column of `x`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc2mPSshbrMM"
      },
      "source": [
        "### Implementation of a Convolutional Autoencoder\n",
        "\n",
        "To demonstrate the use of convolution transpose operations,\n",
        "we will build a **convolutional autoencoder**. Below is an example of a *convolutional* autoencoder that uses solely convolutional layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4oNG1n_RvE1"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential( # like the Composition layer you built\n",
        "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 7)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, 7),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGRvWnjsZzGO"
      },
      "outputs": [],
      "source": [
        "#encoder\n",
        "nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
        "nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
        "nn.Conv2d(32, 64, 7)\n",
        "\n",
        "#decoder\n",
        "nn.ConvTranspose2d(64, 32, 7),\n",
        "nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
        "nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nYbzhJiRvE4"
      },
      "source": [
        "#### Training a Convolutional Autoencoder\n",
        "\n",
        "The training of the convolutional autoencoder will be the same as with the fully-connected autoencoder architecture we introduced in the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46zs6K4uRvE5"
      },
      "outputs": [],
      "source": [
        "def train(model, num_epochs=5, batch_size=64, learning_rate=1e-3):\n",
        "    torch.manual_seed(42)\n",
        "    criterion = nn.MSELoss() # mean square error loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=learning_rate,\n",
        "                                 weight_decay=1e-5)\n",
        "    train_loader = torch.utils.data.DataLoader(mnist_data,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True)\n",
        "    outputs = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for data in train_loader:\n",
        "            img, _ = data\n",
        "            recon = model(img)\n",
        "            loss = criterion(recon, img)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n",
        "        outputs.append((epoch, img, recon),)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y9KKWzjRvE8"
      },
      "source": [
        "Now, we can train this network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhYhq97eRvE8"
      },
      "outputs": [],
      "source": [
        "model = Autoencoder()\n",
        "max_epochs = 20\n",
        "outputs = train(model, num_epochs=max_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqld_9UARvFA"
      },
      "source": [
        "The loss goes down as we train, meaning that our reconstructed images look more\n",
        "and more like the actual images!\n",
        "\n",
        "Let's look at the training progression: that is, the reconstructed images at\n",
        "various points of training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhQMLs9TRvFB"
      },
      "outputs": [],
      "source": [
        "for k in range(0, max_epochs, 5):\n",
        "    plt.figure(figsize=(9, 2))\n",
        "    imgs = outputs[k][1].detach().numpy()\n",
        "    recon = outputs[k][2].detach().numpy()\n",
        "    for i, item in enumerate(imgs):\n",
        "        if i >= 9: break\n",
        "        plt.subplot(2, 9, i+1)\n",
        "        plt.imshow(item[0])\n",
        "\n",
        "    for i, item in enumerate(recon):\n",
        "        if i >= 9: break\n",
        "        plt.subplot(2, 9, 9+i+1)\n",
        "        plt.imshow(item[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uOX8Rzoxnr4"
      },
      "source": [
        "At first, the reconstructed images look nothing like the originals. Rather, the\n",
        "reconstructions look more like the average of some training images.\n",
        "As training progresses, our reconstructions are clearer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWeEQnwvQj1M"
      },
      "source": [
        "## Denoising Autoencoder\n",
        "We can add noise to our data and see if we can train an autoencoder to clean out the noise added to our images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCCQkQR6R-qv"
      },
      "outputs": [],
      "source": [
        "def train(model, num_epochs=5, batch_size=64, learning_rate=1e-3):\n",
        "    torch.manual_seed(42)\n",
        "    criterion = nn.MSELoss() # mean square error loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=learning_rate,\n",
        "                                 weight_decay=1e-5)\n",
        "    train_loader = torch.utils.data.DataLoader(mnist_data,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True)\n",
        "    noise = 0.5\n",
        "    outputs = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for data in train_loader:\n",
        "            img, _ = data\n",
        "\n",
        "            img_noisy = img + noise * torch.randn(*img.shape)\n",
        "            img_noisy = np.clip(img_noisy, 0., 1.)\n",
        "\n",
        "            recon = model(img_noisy)\n",
        "            #img = img.view(-1, 28 * 28)\n",
        "            loss = criterion(recon, img)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n",
        "        outputs.append((epoch, img_noisy, recon),)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxIfjezQcuQh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# train denoising autoencoder\n",
        "model = Autoencoder()\n",
        "max_epochs = 20\n",
        "outputs = train(model, num_epochs=max_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEzp18c5cyaw"
      },
      "outputs": [],
      "source": [
        "# reconstructed images at various parts of training\n",
        "for k in range(0, max_epochs, 5):\n",
        "    plt.figure(figsize=(9, 2))\n",
        "    imgs = outputs[k][1].detach().numpy()\n",
        "    recon = outputs[k][2].detach().numpy()\n",
        "    for i, item in enumerate(imgs):\n",
        "        if i >= 9: break\n",
        "        plt.subplot(2, 9, i+1)\n",
        "        plt.imshow(item[0])\n",
        "\n",
        "    for i, item in enumerate(recon):\n",
        "        if i >= 9: break\n",
        "        plt.subplot(2, 9, 9+i+1)\n",
        "        plt.imshow(item[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H01dBfkd44dz"
      },
      "source": [
        "### Testing on new images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y45YI8CASoxD"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "mnist_data = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_data = list(mnist_data)[4096:4160]\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(mnist_data,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "img, labels = next(dataiter)\n",
        "\n",
        "# add noise to the test images\n",
        "noise = 0.5\n",
        "img_noisy = img + noise * torch.randn(*img.shape)\n",
        "img_noisy = np.clip(img_noisy, 0., 1.)\n",
        "\n",
        "# get sample outputs\n",
        "recon = model(img_noisy)\n",
        "# prep images for display\n",
        "img_noisy = img_noisy.numpy()\n",
        "recon = recon.detach().numpy()\n",
        "\n",
        "# reconstructed images at various parts of training\n",
        "for k in range(1):\n",
        "    plt.figure(figsize=(9, 2))\n",
        "\n",
        "    for i, item in enumerate(img_noisy):\n",
        "        if i >= 9: break\n",
        "        plt.subplot(2, 9, i+1)\n",
        "        plt.imshow(item[0])\n",
        "\n",
        "    for i, item in enumerate(recon):\n",
        "        if i >= 9: break\n",
        "        plt.subplot(2, 9, 9+i+1)\n",
        "        plt.imshow(item[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO-Gyo9FRAH4"
      },
      "source": [
        "Autoencoders are well suited for extracting compressed representations of images and can correct things that don't match the expectation. This approach can be extended to other applications such as handling object occlusion, or filling in missing segments of an image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x50PTc-cRvFD"
      },
      "source": [
        "## Structure in the Embeddings\n",
        "\n",
        "Since we are drastically reducing the dimensionality of the image, there has to be\n",
        "some kind of structure in the embedding space. That is, the network should be able\n",
        "to \"save\" space by mapping similar images to similar embeddings.\n",
        "\n",
        "We will demonstrate the structure of the embedding space by having\n",
        "some fun with our autoencoders. Let's begin with two images in our training set.\n",
        "For now, we'll choose images of the same digit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3vAiDG_eyZ8"
      },
      "source": [
        "First load pre-denoising autoencoder architecture and training although you could also do this with the denosing autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VFCofwQen0D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "mnist_data = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_data = list(mnist_data)[:4096]\n",
        "\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 7)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, 7),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "def train(model, num_epochs=5, batch_size=64, learning_rate=1e-3):\n",
        "    torch.manual_seed(42)\n",
        "    criterion = nn.MSELoss() # mean square error loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=learning_rate,\n",
        "                                 weight_decay=1e-5) # <--\n",
        "    train_loader = torch.utils.data.DataLoader(mnist_data,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=True)\n",
        "    outputs = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for data in train_loader:\n",
        "            img, _ = data\n",
        "            recon = model(img)\n",
        "            loss = criterion(recon, img)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n",
        "        outputs.append((epoch, img, recon),)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrJSAliLeowf"
      },
      "outputs": [],
      "source": [
        "model = Autoencoder()\n",
        "max_epochs = 20\n",
        "outputs = train(model, num_epochs=max_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Nn6IHJ5e9CF"
      },
      "source": [
        "Output two sample images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4IGJ11kRvFE"
      },
      "outputs": [],
      "source": [
        "imgs = outputs[max_epochs-1][1].detach().numpy()\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(imgs[0][0])\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(imgs[8][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2CL1th6RvFH"
      },
      "source": [
        "We will then compute the **low-dimensional embeddings** of both images,\n",
        "by applying the **encoder**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhoIWl7LRvFI"
      },
      "outputs": [],
      "source": [
        "x1 = outputs[max_epochs-1][1][0,:,:,:] # first image\n",
        "x2 = outputs[max_epochs-1][1][8,:,:,:] # second image\n",
        "x = torch.stack([x1,x2])     # stack them together so we only call `encoder` once\n",
        "embedding = model.encoder(x)\n",
        "e1 = embedding[0] # embedding of first image\n",
        "e2 = embedding[1] # embedding of second image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyc2PrGXRvFL"
      },
      "source": [
        "Now we will do something interesting. Not only are we going to run the\n",
        "decoder on those two embeddings `e1` and `e2`, we are also going to **interpolate**\n",
        "between the two embeddings and decode those as well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0hm_VK2RvFM"
      },
      "outputs": [],
      "source": [
        "embedding_values = []\n",
        "for i in range(0, 10):\n",
        "    e = e1 * (i/10) + e2 * (10-i)/10\n",
        "    embedding_values.append(e)\n",
        "embedding_values = torch.stack(embedding_values)\n",
        "\n",
        "recons = model.decoder(embedding_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekqPm_M_RvFO"
      },
      "source": [
        "Let's plot the reconstructions of each interpolated values.\n",
        "The original images are shown below too:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIEzAn1bRvFP"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 2))\n",
        "for i, recon in enumerate(recons.detach().numpy()):\n",
        "    plt.subplot(2,10,i+1)\n",
        "    plt.imshow(recon[0])\n",
        "plt.subplot(2,10,11)\n",
        "plt.imshow(imgs[8][0])\n",
        "plt.subplot(2,10,20)\n",
        "plt.imshow(imgs[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tklMPBFjRvFW"
      },
      "source": [
        "Notice that there is a smooth transition between the two images!\n",
        "The middle images are likely new, in that there are no training images\n",
        "that are exactly like any of the generated images.\n",
        "\n",
        "As promised, we can do the same thing with two images containing\n",
        "different digits. There should be a smooth transition between\n",
        "the two digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6eP-JBTRvFZ"
      },
      "outputs": [],
      "source": [
        "def interpolate(index1, index2):\n",
        "    x1 = mnist_data[index1][0]\n",
        "    x2 = mnist_data[index2][0]\n",
        "    x = torch.stack([x1,x2])\n",
        "    embedding = model.encoder(x)\n",
        "    e1 = embedding[0] # embedding of first image\n",
        "    e2 = embedding[1] # embedding of second image\n",
        "\n",
        "    embedding_values = []\n",
        "    for i in range(0, 10):\n",
        "        e = e1 * (i/10) + e2 * (10-i)/10\n",
        "        embedding_values.append(e)\n",
        "    embedding_values = torch.stack(embedding_values)\n",
        "\n",
        "    recons = model.decoder(embedding_values)\n",
        "\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    for i, recon in enumerate(recons.detach().numpy()):\n",
        "        plt.subplot(2,10,i+1)\n",
        "        plt.imshow(recon[0])\n",
        "    plt.subplot(2,10,11)\n",
        "    plt.imshow(x2[0])\n",
        "    plt.subplot(2,10,20)\n",
        "    plt.imshow(x1[0])\n",
        "\n",
        "interpolate(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cseo0CBxRvFe"
      },
      "outputs": [],
      "source": [
        "interpolate(1, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzlU3VNTRvFg"
      },
      "outputs": [],
      "source": [
        "interpolate(4, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jp2wb1mhRvFi"
      },
      "outputs": [],
      "source": [
        "interpolate(20, 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob2ci_XRRvFl"
      },
      "outputs": [],
      "source": [
        "def interpolate_pixel(index1, index2):\n",
        "    x1 = mnist_data[index1][0]\n",
        "    x2 = mnist_data[index2][0]\n",
        "\n",
        "    interpolated_values = []\n",
        "    for i in range(0, 10):\n",
        "        e = x1 * (i/10) + x2 * (10-i)/10\n",
        "        interpolated_values.append(e)\n",
        "\n",
        "    plt.figure(figsize=(10, 2))\n",
        "    for i, recon in enumerate(interpolated_values):\n",
        "        plt.subplot(2,10,i+1)\n",
        "        plt.imshow(recon[0])\n",
        "    plt.subplot(2,10,11)\n",
        "    plt.imshow(x2[0])\n",
        "    plt.subplot(2,10,20)\n",
        "    plt.imshow(x1[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4b_zv16RvFn"
      },
      "outputs": [],
      "source": [
        "interpolate_pixel(20, 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hP9IYXKdRNV"
      },
      "source": [
        "What happens if we randomly initialize in the embedding space?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STy3fC--RvFp"
      },
      "outputs": [],
      "source": [
        "d = model.decoder(torch.randn(1, 64, 1, 1)).detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-vEvuVvRvFs"
      },
      "outputs": [],
      "source": [
        "d.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRdlHfQQRvFv"
      },
      "outputs": [],
      "source": [
        "plt.imshow(d[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL3UE6BNdfZR"
      },
      "source": [
        "The variational autoencoder will allow us to randomly initialize in the embedding space to generate new MNIST-like samples. Provided below is sample code showing how to do that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxJbLIRSVBFC"
      },
      "source": [
        "## Variational Autoencoder\n",
        "\n",
        "![alt text](http://kvfrans.com/content/images/2016/08/vae.jpg)\n",
        "\n",
        "To allow us to sample from the embedding space and generate new images, we add a constraint on the encoding network that forces it to generate latent vectors that roughly follow a unit Gaussian distribution. This constraint is what separates a variational autoencoder from the ones we've seen up until now.\n",
        "\n",
        "Now generating new images requires that we sample a latent vector from the unit Gaussian and pass it into the decoder.\n",
        "\n",
        "As shown in the figure, we will have encoding and decoding networks similar to what we used before, whether with fully-connected or convolutional layers. Then we add two additional linear layers to hold the mean and standard deviation vectors of the embedding space. We will need some way to generate a sampled latent space which will act as input to the decoding network.\n",
        "\n",
        "We will also need to update our loss function to use Kullback-Leibler divergence to constrain the embedding space to follow a unit Gaussian distribution. You will not be required to know the math behind this.\n",
        "\n",
        "A demonstration of the variational autoencoder is provided below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY8gcmmTB-e7"
      },
      "outputs": [],
      "source": [
        "#Setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# train data\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True,\n",
        "                   transform=transforms.ToTensor()),\n",
        "                   batch_size=64, shuffle=True)\n",
        "\n",
        "# test data\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=64, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_r1uXeYCQLZ"
      },
      "outputs": [],
      "source": [
        "# dimensions of latent space\n",
        "zdim = 25\n",
        "\n",
        "# Variational Autoencoder\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # encoder\n",
        "        self.fc1 = nn.Linear(28 * 28, 350)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2m = nn.Linear(350, zdim)  # mu layer\n",
        "        self.fc2s = nn.Linear(350, zdim)  # sd layer\n",
        "\n",
        "        # decoder\n",
        "        self.fc3 = nn.Linear(zdim, 350)\n",
        "        self.fc4 = nn.Linear(350, 28 * 28)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = self.relu(self.fc1(x))\n",
        "        return self.fc2m(h1), self.fc2s(h1)\n",
        "\n",
        "    # reparameterize\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = std.data.new(std.size()).normal_()\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = self.relu(self.fc3(z))\n",
        "        return self.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 28 * 28))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QE37iWJ1MeMH"
      },
      "outputs": [],
      "source": [
        "# loss function for VAE are unique and use Kullback-Leibler\n",
        "# divergence measure to force distribution to match unit Gaussian\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    bce = F.binary_cross_entropy(recon_x, x.view(-1, 28 * 28))\n",
        "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    kld /= batch_size * 28 * 28\n",
        "    return bce + kld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSxJcz1InUrL"
      },
      "outputs": [],
      "source": [
        "def train(model, num_epochs = 1, batch_size = 64, learning_rate = 1e-3):\n",
        "    model.train() #train mode so that we do reparameterization\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(datasets.MNIST('data',\n",
        "               train=True, download=True, transform=transforms.ToTensor()),\n",
        "               batch_size = batch_size, shuffle = True)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      for data in train_loader:  # load batch\n",
        "          img, _ = data\n",
        "\n",
        "          recon, mu, logvar = model(img)\n",
        "          loss = loss_function(recon, img, mu, logvar) # calculate loss\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "      print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-FNs1LgNFWD"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "model = Autoencoder()\n",
        "train(model, num_epochs = 30, batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBKEHsPGupuo"
      },
      "outputs": [],
      "source": [
        "# generate random samples in latent space\n",
        "model.eval()\n",
        "sample = torch.randn(64, zdim)\n",
        "sample = model.decode(sample)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "imgs = sample.data.view(64, 28, 28).numpy()\n",
        "plt.imshow(imgs[4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfbfFkbgvWdG"
      },
      "outputs": [],
      "source": [
        "# display images\n",
        "for k in range(1):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "\n",
        "    for i, item in enumerate(imgs):\n",
        "        plt.subplot(8, 8, i+1)\n",
        "        plt.imshow(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZaX4Z3vd6qL"
      },
      "source": [
        "In summary we have learned about several different autoencoders. The different architectures we explored with stacked, convolutional, denoising, and variational autoencoders can be combined to take advantage of their strengths and weaknesses to develop an architecture best suited for your problem.\n",
        "\n",
        "In this tutorial we didn't go over **semi-supervised learning** which is another very practical application of autoencoders. By learning embeddings from unlabeled data, autoencoders can improve model performance in situations where labeled data may be scarce."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}